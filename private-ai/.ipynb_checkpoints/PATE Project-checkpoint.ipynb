{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tf_encrypted:Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow (1.13.1). Fix this by compiling custom ops.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.current_device() # Bug in pytorch with GPUs on interactive shells and jupyter - workaround by calling this first\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.nn import NLLLoss, Sequential, functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from syft.frameworks.torch.differential_privacy import pate\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True)\n",
    "train_data = mnist_trainset.data\n",
    "train_mean, train_std = train_data.float().mean(), train_data.float().std()\n",
    "train_targets = mnist_trainset.targets\n",
    "\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True)\n",
    "test_data = mnist_testset.data\n",
    "test_mean, test_std = test_data.float().mean(), test_data.float().std()\n",
    "test_targets = mnist_testset.targets\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transformer = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_trainset.transform = transformer\n",
    "mnist_testset.transform = transformer\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=16, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(mnist_testset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teachers = {}\n",
    "tv_models = {'vgg19_bn': models.vgg19_bn,\n",
    "             'resnet50': models.resnet50,\n",
    "             'resnet18': models.resnet18,\n",
    "             'inception_v3': models.inception_v3,\n",
    "             'densnet121': models.densenet121,\n",
    "             'squeezenet1_1': models.squeezenet1_1}\n",
    "for key in tv_models:\n",
    "    model = tv_models[key](pretrained=True)\n",
    "    model.to(device)\n",
    "    teachers[key] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in zip(teachers.keys(),teachers.values()):\n",
    "    x = Sequential(*list(model.children()))[-1]\n",
    "    print(name, x)\n",
    "#     if type(x) == Sequential:\n",
    "#         print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teachers['resnet50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistResNet(models.resnet.ResNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(models.resnet.Bottleneck, [3, 4, 6, 3], num_classes=10)\n",
    "        self.conv1 = torch.nn.Conv2d(1, 64, \n",
    "            kernel_size=(7, 7),\n",
    "            stride=(2, 2), \n",
    "            padding=(3, 3), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.num_inputs = 784\n",
    "        self.num_classes = 10\n",
    "        self.create_layers()\n",
    "        \n",
    "    def create_layers(self):\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "                # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL  0\n",
      "\tEpoch 1/2.. \n",
      "\t\tTrain loss: 1.548.. \t\tTest loss: 0.818.. \t\tTest accuracy: 0.726\n",
      "\t\tTrain loss: 0.704.. \t\tTest loss: 0.593.. \t\tTest accuracy: 0.812\n",
      "\t\tTrain loss: 0.569.. \t\tTest loss: 0.468.. \t\tTest accuracy: 0.852\n",
      "\t\tTrain loss: 0.466.. \t\tTest loss: 0.395.. \t\tTest accuracy: 0.879\n",
      "\t\tTrain loss: 0.459.. \t\tTest loss: 0.418.. \t\tTest accuracy: 0.875\n",
      "\t\tTrain loss: 0.438.. \t\tTest loss: 0.427.. \t\tTest accuracy: 0.864\n",
      "\t\tTrain loss: 0.396.. \t\tTest loss: 0.372.. \t\tTest accuracy: 0.877\n",
      "\t\tTrain loss: 0.389.. \t\tTest loss: 0.323.. \t\tTest accuracy: 0.900\n",
      "\t\tTrain loss: 0.374.. \t\tTest loss: 0.318.. \t\tTest accuracy: 0.901\n",
      "\t\tTrain loss: 0.309.. \t\tTest loss: 0.310.. \t\tTest accuracy: 0.903\n",
      "\t\tTrain loss: 0.301.. \t\tTest loss: 0.306.. \t\tTest accuracy: 0.905\n",
      "\t\tTrain loss: 0.282.. \t\tTest loss: 0.309.. \t\tTest accuracy: 0.904\n",
      "\t\tTrain loss: 0.221.. \t\tTest loss: 0.282.. \t\tTest accuracy: 0.914\n",
      "\t\tTrain loss: 0.291.. \t\tTest loss: 0.292.. \t\tTest accuracy: 0.906\n",
      "\t\tTrain loss: 0.297.. \t\tTest loss: 0.232.. \t\tTest accuracy: 0.927\n",
      "\t\tTrain loss: 0.288.. \t\tTest loss: 0.244.. \t\tTest accuracy: 0.924\n",
      "\t\tTrain loss: 0.272.. \t\tTest loss: 0.240.. \t\tTest accuracy: 0.923\n",
      "\t\tTrain loss: 0.235.. \t\tTest loss: 0.264.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.248.. \t\tTest loss: 0.204.. \t\tTest accuracy: 0.935\n",
      "\t\tTrain loss: 0.258.. \t\tTest loss: 0.217.. \t\tTest accuracy: 0.932\n",
      "\t\tTrain loss: 0.225.. \t\tTest loss: 0.221.. \t\tTest accuracy: 0.930\n",
      "\t\tTrain loss: 0.259.. \t\tTest loss: 0.202.. \t\tTest accuracy: 0.936\n",
      "\t\tTrain loss: 0.230.. \t\tTest loss: 0.210.. \t\tTest accuracy: 0.935\n",
      "\t\tTrain loss: 0.248.. \t\tTest loss: 0.189.. \t\tTest accuracy: 0.942\n",
      "\t\tTrain loss: 0.211.. \t\tTest loss: 0.215.. \t\tTest accuracy: 0.932\n",
      "\t\tTrain loss: 0.251.. \t\tTest loss: 0.211.. \t\tTest accuracy: 0.933\n",
      "\t\tTrain loss: 0.216.. \t\tTest loss: 0.220.. \t\tTest accuracy: 0.929\n",
      "\t\tTrain loss: 0.217.. \t\tTest loss: 0.199.. \t\tTest accuracy: 0.938\n",
      "\t\tTrain loss: 0.200.. \t\tTest loss: 0.232.. \t\tTest accuracy: 0.926\n",
      "\t\tTrain loss: 0.198.. \t\tTest loss: 0.227.. \t\tTest accuracy: 0.926\n",
      "\t\tTrain loss: 0.193.. \t\tTest loss: 0.225.. \t\tTest accuracy: 0.928\n",
      "\t\tTrain loss: 0.207.. \t\tTest loss: 0.211.. \t\tTest accuracy: 0.933\n",
      "\t\tTrain loss: 0.232.. \t\tTest loss: 0.163.. \t\tTest accuracy: 0.949\n",
      "\t\tTrain loss: 0.171.. \t\tTest loss: 0.191.. \t\tTest accuracy: 0.941\n",
      "\t\tTrain loss: 0.182.. \t\tTest loss: 0.165.. \t\tTest accuracy: 0.950\n",
      "\t\tTrain loss: 0.215.. \t\tTest loss: 0.193.. \t\tTest accuracy: 0.941\n",
      "\t\tTrain loss: 0.182.. \t\tTest loss: 0.190.. \t\tTest accuracy: 0.939\n",
      "TRAINING MODEL  0\n",
      "\tEpoch 2/2.. \n",
      "\t\tTrain loss: 0.192.. \t\tTest loss: 0.166.. \t\tTest accuracy: 0.947\n",
      "\t\tTrain loss: 0.188.. \t\tTest loss: 0.165.. \t\tTest accuracy: 0.949\n",
      "\t\tTrain loss: 0.168.. \t\tTest loss: 0.242.. \t\tTest accuracy: 0.924\n",
      "\t\tTrain loss: 0.165.. \t\tTest loss: 0.170.. \t\tTest accuracy: 0.948\n",
      "\t\tTrain loss: 0.194.. \t\tTest loss: 0.212.. \t\tTest accuracy: 0.934\n",
      "\t\tTrain loss: 0.178.. \t\tTest loss: 0.148.. \t\tTest accuracy: 0.954\n",
      "\t\tTrain loss: 0.169.. \t\tTest loss: 0.154.. \t\tTest accuracy: 0.954\n",
      "\t\tTrain loss: 0.160.. \t\tTest loss: 0.187.. \t\tTest accuracy: 0.941\n",
      "\t\tTrain loss: 0.155.. \t\tTest loss: 0.153.. \t\tTest accuracy: 0.952\n",
      "\t\tTrain loss: 0.169.. \t\tTest loss: 0.144.. \t\tTest accuracy: 0.955\n",
      "\t\tTrain loss: 0.169.. \t\tTest loss: 0.195.. \t\tTest accuracy: 0.940\n",
      "\t\tTrain loss: 0.164.. \t\tTest loss: 0.180.. \t\tTest accuracy: 0.944\n",
      "\t\tTrain loss: 0.161.. \t\tTest loss: 0.229.. \t\tTest accuracy: 0.930\n",
      "\t\tTrain loss: 0.176.. \t\tTest loss: 0.165.. \t\tTest accuracy: 0.950\n",
      "\t\tTrain loss: 0.162.. \t\tTest loss: 0.153.. \t\tTest accuracy: 0.954\n",
      "\t\tTrain loss: 0.146.. \t\tTest loss: 0.193.. \t\tTest accuracy: 0.939\n",
      "\t\tTrain loss: 0.147.. \t\tTest loss: 0.133.. \t\tTest accuracy: 0.959\n",
      "\t\tTrain loss: 0.145.. \t\tTest loss: 0.149.. \t\tTest accuracy: 0.955\n",
      "\t\tTrain loss: 0.104.. \t\tTest loss: 0.158.. \t\tTest accuracy: 0.953\n",
      "\t\tTrain loss: 0.172.. \t\tTest loss: 0.161.. \t\tTest accuracy: 0.953\n",
      "\t\tTrain loss: 0.159.. \t\tTest loss: 0.171.. \t\tTest accuracy: 0.949\n",
      "\t\tTrain loss: 0.187.. \t\tTest loss: 0.149.. \t\tTest accuracy: 0.956\n",
      "\t\tTrain loss: 0.156.. \t\tTest loss: 0.152.. \t\tTest accuracy: 0.954\n",
      "\t\tTrain loss: 0.150.. \t\tTest loss: 0.135.. \t\tTest accuracy: 0.959\n",
      "\t\tTrain loss: 0.180.. \t\tTest loss: 0.153.. \t\tTest accuracy: 0.954\n",
      "\t\tTrain loss: 0.161.. \t\tTest loss: 0.164.. \t\tTest accuracy: 0.950\n",
      "\t\tTrain loss: 0.147.. \t\tTest loss: 0.157.. \t\tTest accuracy: 0.952\n",
      "\t\tTrain loss: 0.150.. \t\tTest loss: 0.131.. \t\tTest accuracy: 0.960\n",
      "\t\tTrain loss: 0.140.. \t\tTest loss: 0.188.. \t\tTest accuracy: 0.942\n",
      "\t\tTrain loss: 0.150.. \t\tTest loss: 0.135.. \t\tTest accuracy: 0.958\n",
      "\t\tTrain loss: 0.140.. \t\tTest loss: 0.150.. \t\tTest accuracy: 0.953\n",
      "\t\tTrain loss: 0.164.. \t\tTest loss: 0.157.. \t\tTest accuracy: 0.952\n",
      "\t\tTrain loss: 0.151.. \t\tTest loss: 0.150.. \t\tTest accuracy: 0.954\n",
      "\t\tTrain loss: 0.166.. \t\tTest loss: 0.156.. \t\tTest accuracy: 0.954\n",
      "\t\tTrain loss: 0.153.. \t\tTest loss: 0.166.. \t\tTest accuracy: 0.949\n",
      "\t\tTrain loss: 0.175.. \t\tTest loss: 0.133.. \t\tTest accuracy: 0.959\n",
      "\t\tTrain loss: 0.143.. \t\tTest loss: 0.165.. \t\tTest accuracy: 0.947\n",
      "\t\tTrain loss: 0.171.. \t\tTest loss: 0.188.. \t\tTest accuracy: 0.939\n",
      "TRAINING MODEL  1\n",
      "\tEpoch 1/4.. \n",
      "\t\tTrain loss: 1.514.. \t\tTest loss: 1.110.. \t\tTest accuracy: 0.645\n",
      "\t\tTrain loss: 1.008.. \t\tTest loss: 0.917.. \t\tTest accuracy: 0.667\n",
      "\t\tTrain loss: 0.841.. \t\tTest loss: 0.757.. \t\tTest accuracy: 0.781\n",
      "\t\tTrain loss: 0.718.. \t\tTest loss: 0.585.. \t\tTest accuracy: 0.836\n",
      "\t\tTrain loss: 0.626.. \t\tTest loss: 0.612.. \t\tTest accuracy: 0.823\n",
      "\t\tTrain loss: 0.653.. \t\tTest loss: 0.622.. \t\tTest accuracy: 0.796\n",
      "\t\tTrain loss: 0.597.. \t\tTest loss: 0.589.. \t\tTest accuracy: 0.831\n",
      "\t\tTrain loss: 0.575.. \t\tTest loss: 0.576.. \t\tTest accuracy: 0.845\n",
      "\t\tTrain loss: 0.487.. \t\tTest loss: 0.595.. \t\tTest accuracy: 0.815\n",
      "\t\tTrain loss: 0.537.. \t\tTest loss: 0.523.. \t\tTest accuracy: 0.850\n",
      "\t\tTrain loss: 0.549.. \t\tTest loss: 0.506.. \t\tTest accuracy: 0.849\n",
      "\t\tTrain loss: 0.513.. \t\tTest loss: 0.503.. \t\tTest accuracy: 0.853\n",
      "\t\tTrain loss: 0.571.. \t\tTest loss: 0.523.. \t\tTest accuracy: 0.862\n",
      "\t\tTrain loss: 0.536.. \t\tTest loss: 0.500.. \t\tTest accuracy: 0.866\n",
      "\t\tTrain loss: 0.574.. \t\tTest loss: 0.443.. \t\tTest accuracy: 0.881\n",
      "\t\tTrain loss: 0.540.. \t\tTest loss: 0.539.. \t\tTest accuracy: 0.855\n",
      "\t\tTrain loss: 0.542.. \t\tTest loss: 0.408.. \t\tTest accuracy: 0.879\n",
      "\t\tTrain loss: 0.437.. \t\tTest loss: 0.690.. \t\tTest accuracy: 0.820\n",
      "\t\tTrain loss: 0.454.. \t\tTest loss: 0.433.. \t\tTest accuracy: 0.874\n",
      "\t\tTrain loss: 0.515.. \t\tTest loss: 0.506.. \t\tTest accuracy: 0.863\n",
      "\t\tTrain loss: 0.497.. \t\tTest loss: 0.469.. \t\tTest accuracy: 0.860\n",
      "\t\tTrain loss: 0.521.. \t\tTest loss: 0.506.. \t\tTest accuracy: 0.860\n",
      "\t\tTrain loss: 0.503.. \t\tTest loss: 0.567.. \t\tTest accuracy: 0.827\n",
      "\t\tTrain loss: 0.507.. \t\tTest loss: 0.533.. \t\tTest accuracy: 0.849\n",
      "\t\tTrain loss: 0.572.. \t\tTest loss: 0.431.. \t\tTest accuracy: 0.885\n",
      "\t\tTrain loss: 0.493.. \t\tTest loss: 0.396.. \t\tTest accuracy: 0.887\n",
      "\t\tTrain loss: 0.440.. \t\tTest loss: 0.385.. \t\tTest accuracy: 0.896\n",
      "\t\tTrain loss: 0.413.. \t\tTest loss: 0.402.. \t\tTest accuracy: 0.898\n",
      "\t\tTrain loss: 0.460.. \t\tTest loss: 0.524.. \t\tTest accuracy: 0.856\n",
      "\t\tTrain loss: 0.421.. \t\tTest loss: 0.516.. \t\tTest accuracy: 0.862\n",
      "\t\tTrain loss: 0.460.. \t\tTest loss: 0.392.. \t\tTest accuracy: 0.893\n",
      "\t\tTrain loss: 0.461.. \t\tTest loss: 0.370.. \t\tTest accuracy: 0.902\n",
      "\t\tTrain loss: 0.442.. \t\tTest loss: 0.399.. \t\tTest accuracy: 0.893\n",
      "\t\tTrain loss: 0.391.. \t\tTest loss: 0.452.. \t\tTest accuracy: 0.885\n",
      "\t\tTrain loss: 0.472.. \t\tTest loss: 0.411.. \t\tTest accuracy: 0.888\n",
      "\t\tTrain loss: 0.419.. \t\tTest loss: 0.436.. \t\tTest accuracy: 0.883\n",
      "\t\tTrain loss: 0.424.. \t\tTest loss: 0.443.. \t\tTest accuracy: 0.890\n",
      "TRAINING MODEL  1\n",
      "\tEpoch 2/4.. \n",
      "\t\tTrain loss: 0.439.. \t\tTest loss: 0.403.. \t\tTest accuracy: 0.892\n",
      "\t\tTrain loss: 0.469.. \t\tTest loss: 0.543.. \t\tTest accuracy: 0.871\n",
      "\t\tTrain loss: 0.412.. \t\tTest loss: 0.513.. \t\tTest accuracy: 0.875\n",
      "\t\tTrain loss: 0.443.. \t\tTest loss: 0.495.. \t\tTest accuracy: 0.872\n",
      "\t\tTrain loss: 0.373.. \t\tTest loss: 0.391.. \t\tTest accuracy: 0.906\n",
      "\t\tTrain loss: 0.433.. \t\tTest loss: 0.478.. \t\tTest accuracy: 0.881\n",
      "\t\tTrain loss: 0.550.. \t\tTest loss: 0.582.. \t\tTest accuracy: 0.845\n",
      "\t\tTrain loss: 0.372.. \t\tTest loss: 0.396.. \t\tTest accuracy: 0.901\n",
      "\t\tTrain loss: 0.399.. \t\tTest loss: 0.386.. \t\tTest accuracy: 0.897\n",
      "\t\tTrain loss: 0.408.. \t\tTest loss: 0.449.. \t\tTest accuracy: 0.877\n",
      "\t\tTrain loss: 0.436.. \t\tTest loss: 0.415.. \t\tTest accuracy: 0.893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTrain loss: 0.412.. \t\tTest loss: 0.583.. \t\tTest accuracy: 0.833\n",
      "\t\tTrain loss: 0.443.. \t\tTest loss: 0.404.. \t\tTest accuracy: 0.894\n",
      "\t\tTrain loss: 0.451.. \t\tTest loss: 0.398.. \t\tTest accuracy: 0.894\n",
      "\t\tTrain loss: 0.393.. \t\tTest loss: 0.402.. \t\tTest accuracy: 0.900\n",
      "\t\tTrain loss: 0.364.. \t\tTest loss: 0.361.. \t\tTest accuracy: 0.905\n",
      "\t\tTrain loss: 0.440.. \t\tTest loss: 0.465.. \t\tTest accuracy: 0.874\n",
      "\t\tTrain loss: 0.404.. \t\tTest loss: 0.388.. \t\tTest accuracy: 0.906\n",
      "\t\tTrain loss: 0.424.. \t\tTest loss: 0.328.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.400.. \t\tTest loss: 0.633.. \t\tTest accuracy: 0.826\n",
      "\t\tTrain loss: 0.486.. \t\tTest loss: 0.407.. \t\tTest accuracy: 0.898\n",
      "\t\tTrain loss: 0.380.. \t\tTest loss: 0.354.. \t\tTest accuracy: 0.909\n",
      "\t\tTrain loss: 0.450.. \t\tTest loss: 0.418.. \t\tTest accuracy: 0.894\n",
      "\t\tTrain loss: 0.492.. \t\tTest loss: 0.377.. \t\tTest accuracy: 0.898\n",
      "\t\tTrain loss: 0.429.. \t\tTest loss: 0.434.. \t\tTest accuracy: 0.887\n",
      "\t\tTrain loss: 0.440.. \t\tTest loss: 0.390.. \t\tTest accuracy: 0.903\n",
      "\t\tTrain loss: 0.419.. \t\tTest loss: 0.369.. \t\tTest accuracy: 0.903\n",
      "\t\tTrain loss: 0.321.. \t\tTest loss: 0.371.. \t\tTest accuracy: 0.908\n",
      "\t\tTrain loss: 0.395.. \t\tTest loss: 0.356.. \t\tTest accuracy: 0.909\n",
      "\t\tTrain loss: 0.396.. \t\tTest loss: 0.403.. \t\tTest accuracy: 0.895\n",
      "\t\tTrain loss: 0.353.. \t\tTest loss: 0.408.. \t\tTest accuracy: 0.889\n",
      "\t\tTrain loss: 0.414.. \t\tTest loss: 0.393.. \t\tTest accuracy: 0.902\n",
      "\t\tTrain loss: 0.441.. \t\tTest loss: 0.414.. \t\tTest accuracy: 0.887\n",
      "\t\tTrain loss: 0.392.. \t\tTest loss: 0.479.. \t\tTest accuracy: 0.872\n",
      "\t\tTrain loss: 0.410.. \t\tTest loss: 0.424.. \t\tTest accuracy: 0.898\n",
      "\t\tTrain loss: 0.379.. \t\tTest loss: 0.341.. \t\tTest accuracy: 0.913\n",
      "\t\tTrain loss: 0.466.. \t\tTest loss: 0.364.. \t\tTest accuracy: 0.901\n",
      "\t\tTrain loss: 0.421.. \t\tTest loss: 0.416.. \t\tTest accuracy: 0.895\n",
      "TRAINING MODEL  1\n",
      "\tEpoch 3/4.. \n",
      "\t\tTrain loss: 0.322.. \t\tTest loss: 0.502.. \t\tTest accuracy: 0.877\n",
      "\t\tTrain loss: 0.405.. \t\tTest loss: 0.347.. \t\tTest accuracy: 0.912\n",
      "\t\tTrain loss: 0.450.. \t\tTest loss: 0.397.. \t\tTest accuracy: 0.901\n",
      "\t\tTrain loss: 0.419.. \t\tTest loss: 0.481.. \t\tTest accuracy: 0.861\n",
      "\t\tTrain loss: 0.468.. \t\tTest loss: 0.777.. \t\tTest accuracy: 0.759\n",
      "\t\tTrain loss: 0.466.. \t\tTest loss: 0.494.. \t\tTest accuracy: 0.871\n",
      "\t\tTrain loss: 0.488.. \t\tTest loss: 0.428.. \t\tTest accuracy: 0.892\n",
      "\t\tTrain loss: 0.585.. \t\tTest loss: 0.384.. \t\tTest accuracy: 0.900\n",
      "\t\tTrain loss: 0.381.. \t\tTest loss: 0.352.. \t\tTest accuracy: 0.905\n",
      "\t\tTrain loss: 0.439.. \t\tTest loss: 0.356.. \t\tTest accuracy: 0.905\n",
      "\t\tTrain loss: 0.339.. \t\tTest loss: 0.372.. \t\tTest accuracy: 0.909\n",
      "\t\tTrain loss: 0.369.. \t\tTest loss: 0.336.. \t\tTest accuracy: 0.911\n",
      "\t\tTrain loss: 0.440.. \t\tTest loss: 0.377.. \t\tTest accuracy: 0.897\n",
      "\t\tTrain loss: 0.394.. \t\tTest loss: 0.407.. \t\tTest accuracy: 0.888\n",
      "\t\tTrain loss: 0.458.. \t\tTest loss: 0.400.. \t\tTest accuracy: 0.898\n",
      "\t\tTrain loss: 0.402.. \t\tTest loss: 0.413.. \t\tTest accuracy: 0.904\n",
      "\t\tTrain loss: 0.423.. \t\tTest loss: 0.376.. \t\tTest accuracy: 0.904\n",
      "\t\tTrain loss: 0.340.. \t\tTest loss: 0.366.. \t\tTest accuracy: 0.907\n",
      "\t\tTrain loss: 0.430.. \t\tTest loss: 0.397.. \t\tTest accuracy: 0.893\n",
      "\t\tTrain loss: 0.357.. \t\tTest loss: 0.374.. \t\tTest accuracy: 0.903\n",
      "\t\tTrain loss: 0.340.. \t\tTest loss: 0.336.. \t\tTest accuracy: 0.917\n",
      "\t\tTrain loss: 0.375.. \t\tTest loss: 0.388.. \t\tTest accuracy: 0.902\n",
      "\t\tTrain loss: 0.413.. \t\tTest loss: 0.397.. \t\tTest accuracy: 0.901\n",
      "\t\tTrain loss: 0.444.. \t\tTest loss: 0.514.. \t\tTest accuracy: 0.891\n",
      "\t\tTrain loss: 0.411.. \t\tTest loss: 0.423.. \t\tTest accuracy: 0.891\n",
      "\t\tTrain loss: 0.374.. \t\tTest loss: 0.378.. \t\tTest accuracy: 0.907\n",
      "\t\tTrain loss: 0.469.. \t\tTest loss: 0.487.. \t\tTest accuracy: 0.882\n",
      "\t\tTrain loss: 0.396.. \t\tTest loss: 0.441.. \t\tTest accuracy: 0.896\n",
      "\t\tTrain loss: 0.334.. \t\tTest loss: 0.344.. \t\tTest accuracy: 0.912\n",
      "\t\tTrain loss: 0.395.. \t\tTest loss: 0.373.. \t\tTest accuracy: 0.904\n",
      "\t\tTrain loss: 0.364.. \t\tTest loss: 0.342.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.503.. \t\tTest loss: 0.419.. \t\tTest accuracy: 0.890\n",
      "\t\tTrain loss: 0.368.. \t\tTest loss: 0.369.. \t\tTest accuracy: 0.902\n",
      "\t\tTrain loss: 0.344.. \t\tTest loss: 0.391.. \t\tTest accuracy: 0.903\n",
      "\t\tTrain loss: 0.465.. \t\tTest loss: 0.378.. \t\tTest accuracy: 0.901\n",
      "\t\tTrain loss: 0.416.. \t\tTest loss: 0.393.. \t\tTest accuracy: 0.892\n",
      "\t\tTrain loss: 0.421.. \t\tTest loss: 0.336.. \t\tTest accuracy: 0.905\n",
      "TRAINING MODEL  1\n",
      "\tEpoch 4/4.. \n",
      "\t\tTrain loss: 0.434.. \t\tTest loss: 0.390.. \t\tTest accuracy: 0.900\n",
      "\t\tTrain loss: 0.431.. \t\tTest loss: 0.379.. \t\tTest accuracy: 0.899\n",
      "\t\tTrain loss: 0.357.. \t\tTest loss: 0.353.. \t\tTest accuracy: 0.918\n",
      "\t\tTrain loss: 0.368.. \t\tTest loss: 0.376.. \t\tTest accuracy: 0.911\n",
      "\t\tTrain loss: 0.356.. \t\tTest loss: 0.390.. \t\tTest accuracy: 0.910\n",
      "\t\tTrain loss: 0.372.. \t\tTest loss: 0.430.. \t\tTest accuracy: 0.896\n",
      "\t\tTrain loss: 0.366.. \t\tTest loss: 0.406.. \t\tTest accuracy: 0.892\n",
      "\t\tTrain loss: 0.410.. \t\tTest loss: 0.362.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.370.. \t\tTest loss: 0.419.. \t\tTest accuracy: 0.891\n",
      "\t\tTrain loss: 0.417.. \t\tTest loss: 0.360.. \t\tTest accuracy: 0.907\n",
      "\t\tTrain loss: 0.367.. \t\tTest loss: 0.389.. \t\tTest accuracy: 0.902\n",
      "\t\tTrain loss: 0.411.. \t\tTest loss: 0.433.. \t\tTest accuracy: 0.872\n",
      "\t\tTrain loss: 0.392.. \t\tTest loss: 0.327.. \t\tTest accuracy: 0.910\n",
      "\t\tTrain loss: 0.437.. \t\tTest loss: 0.395.. \t\tTest accuracy: 0.897\n",
      "\t\tTrain loss: 0.480.. \t\tTest loss: 0.439.. \t\tTest accuracy: 0.889\n",
      "\t\tTrain loss: 0.425.. \t\tTest loss: 0.496.. \t\tTest accuracy: 0.870\n",
      "\t\tTrain loss: 0.532.. \t\tTest loss: 0.438.. \t\tTest accuracy: 0.882\n",
      "\t\tTrain loss: 0.469.. \t\tTest loss: 0.472.. \t\tTest accuracy: 0.874\n",
      "\t\tTrain loss: 0.458.. \t\tTest loss: 0.496.. \t\tTest accuracy: 0.861\n",
      "\t\tTrain loss: 0.375.. \t\tTest loss: 0.416.. \t\tTest accuracy: 0.891\n",
      "\t\tTrain loss: 0.432.. \t\tTest loss: 0.401.. \t\tTest accuracy: 0.893\n",
      "\t\tTrain loss: 0.368.. \t\tTest loss: 0.333.. \t\tTest accuracy: 0.912\n",
      "\t\tTrain loss: 0.529.. \t\tTest loss: 0.455.. \t\tTest accuracy: 0.890\n",
      "\t\tTrain loss: 0.396.. \t\tTest loss: 0.409.. \t\tTest accuracy: 0.898\n",
      "\t\tTrain loss: 0.374.. \t\tTest loss: 0.343.. \t\tTest accuracy: 0.909\n",
      "\t\tTrain loss: 0.359.. \t\tTest loss: 0.369.. \t\tTest accuracy: 0.904\n",
      "\t\tTrain loss: 0.414.. \t\tTest loss: 0.339.. \t\tTest accuracy: 0.906\n",
      "\t\tTrain loss: 0.332.. \t\tTest loss: 0.354.. \t\tTest accuracy: 0.901\n",
      "\t\tTrain loss: 0.389.. \t\tTest loss: 0.381.. \t\tTest accuracy: 0.906\n",
      "\t\tTrain loss: 0.467.. \t\tTest loss: 0.405.. \t\tTest accuracy: 0.897\n",
      "\t\tTrain loss: 0.463.. \t\tTest loss: 0.563.. \t\tTest accuracy: 0.837\n",
      "\t\tTrain loss: 0.466.. \t\tTest loss: 0.417.. \t\tTest accuracy: 0.898\n",
      "\t\tTrain loss: 0.378.. \t\tTest loss: 0.346.. \t\tTest accuracy: 0.907\n",
      "\t\tTrain loss: 0.376.. \t\tTest loss: 0.432.. \t\tTest accuracy: 0.887\n",
      "\t\tTrain loss: 0.388.. \t\tTest loss: 0.496.. \t\tTest accuracy: 0.888\n",
      "\t\tTrain loss: 0.439.. \t\tTest loss: 0.341.. \t\tTest accuracy: 0.907\n",
      "\t\tTrain loss: 0.341.. \t\tTest loss: 0.390.. \t\tTest accuracy: 0.897\n",
      "\t\tTrain loss: 0.337.. \t\tTest loss: 0.351.. \t\tTest accuracy: 0.913\n",
      "TRAINING MODEL  2\n",
      "\tEpoch 1/4.. \n",
      "\t\tTrain loss: 1.475.. \t\tTest loss: 0.728.. \t\tTest accuracy: 0.768\n",
      "\t\tTrain loss: 0.643.. \t\tTest loss: 0.506.. \t\tTest accuracy: 0.839\n",
      "\t\tTrain loss: 0.553.. \t\tTest loss: 0.522.. \t\tTest accuracy: 0.834\n",
      "\t\tTrain loss: 0.499.. \t\tTest loss: 0.440.. \t\tTest accuracy: 0.859\n",
      "\t\tTrain loss: 0.431.. \t\tTest loss: 0.441.. \t\tTest accuracy: 0.857\n",
      "\t\tTrain loss: 0.431.. \t\tTest loss: 0.400.. \t\tTest accuracy: 0.872\n",
      "\t\tTrain loss: 0.380.. \t\tTest loss: 0.359.. \t\tTest accuracy: 0.892\n",
      "\t\tTrain loss: 0.363.. \t\tTest loss: 0.332.. \t\tTest accuracy: 0.897\n",
      "\t\tTrain loss: 0.365.. \t\tTest loss: 0.289.. \t\tTest accuracy: 0.911\n",
      "\t\tTrain loss: 0.327.. \t\tTest loss: 0.286.. \t\tTest accuracy: 0.914\n",
      "\t\tTrain loss: 0.337.. \t\tTest loss: 0.291.. \t\tTest accuracy: 0.911\n",
      "\t\tTrain loss: 0.307.. \t\tTest loss: 0.271.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.317.. \t\tTest loss: 0.282.. \t\tTest accuracy: 0.915\n",
      "\t\tTrain loss: 0.276.. \t\tTest loss: 0.269.. \t\tTest accuracy: 0.912\n",
      "\t\tTrain loss: 0.314.. \t\tTest loss: 0.274.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.297.. \t\tTest loss: 0.268.. \t\tTest accuracy: 0.917\n",
      "\t\tTrain loss: 0.284.. \t\tTest loss: 0.284.. \t\tTest accuracy: 0.910\n",
      "\t\tTrain loss: 0.218.. \t\tTest loss: 0.228.. \t\tTest accuracy: 0.932\n",
      "\t\tTrain loss: 0.237.. \t\tTest loss: 0.265.. \t\tTest accuracy: 0.921\n",
      "\t\tTrain loss: 0.273.. \t\tTest loss: 0.222.. \t\tTest accuracy: 0.929\n",
      "\t\tTrain loss: 0.247.. \t\tTest loss: 0.260.. \t\tTest accuracy: 0.921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTrain loss: 0.248.. \t\tTest loss: 0.200.. \t\tTest accuracy: 0.939\n",
      "\t\tTrain loss: 0.243.. \t\tTest loss: 0.258.. \t\tTest accuracy: 0.917\n",
      "\t\tTrain loss: 0.244.. \t\tTest loss: 0.238.. \t\tTest accuracy: 0.924\n",
      "\t\tTrain loss: 0.269.. \t\tTest loss: 0.325.. \t\tTest accuracy: 0.902\n",
      "\t\tTrain loss: 0.235.. \t\tTest loss: 0.237.. \t\tTest accuracy: 0.925\n",
      "\t\tTrain loss: 0.187.. \t\tTest loss: 0.198.. \t\tTest accuracy: 0.940\n",
      "\t\tTrain loss: 0.231.. \t\tTest loss: 0.246.. \t\tTest accuracy: 0.921\n",
      "\t\tTrain loss: 0.262.. \t\tTest loss: 0.189.. \t\tTest accuracy: 0.941\n",
      "\t\tTrain loss: 0.194.. \t\tTest loss: 0.195.. \t\tTest accuracy: 0.938\n",
      "\t\tTrain loss: 0.179.. \t\tTest loss: 0.220.. \t\tTest accuracy: 0.931\n",
      "\t\tTrain loss: 0.211.. \t\tTest loss: 0.183.. \t\tTest accuracy: 0.943\n",
      "\t\tTrain loss: 0.198.. \t\tTest loss: 0.167.. \t\tTest accuracy: 0.949\n",
      "\t\tTrain loss: 0.193.. \t\tTest loss: 0.202.. \t\tTest accuracy: 0.937\n",
      "\t\tTrain loss: 0.178.. \t\tTest loss: 0.205.. \t\tTest accuracy: 0.935\n",
      "\t\tTrain loss: 0.237.. \t\tTest loss: 0.212.. \t\tTest accuracy: 0.934\n",
      "\t\tTrain loss: 0.220.. \t\tTest loss: 0.172.. \t\tTest accuracy: 0.947\n",
      "TRAINING MODEL  2\n",
      "\tEpoch 2/4.. \n",
      "\t\tTrain loss: 0.178.. \t\tTest loss: 0.189.. \t\tTest accuracy: 0.941\n",
      "\t\tTrain loss: 0.165.. \t\tTest loss: 0.170.. \t\tTest accuracy: 0.947\n",
      "\t\tTrain loss: 0.164.. \t\tTest loss: 0.173.. \t\tTest accuracy: 0.948\n",
      "\t\tTrain loss: 0.168.. \t\tTest loss: 0.193.. \t\tTest accuracy: 0.938\n",
      "\t\tTrain loss: 0.177.. \t\tTest loss: 0.185.. \t\tTest accuracy: 0.942\n",
      "\t\tTrain loss: 0.168.. \t\tTest loss: 0.154.. \t\tTest accuracy: 0.950\n",
      "\t\tTrain loss: 0.209.. \t\tTest loss: 0.168.. \t\tTest accuracy: 0.949\n",
      "\t\tTrain loss: 0.163.. \t\tTest loss: 0.194.. \t\tTest accuracy: 0.941\n",
      "\t\tTrain loss: 0.163.. \t\tTest loss: 0.163.. \t\tTest accuracy: 0.950\n",
      "\t\tTrain loss: 0.164.. \t\tTest loss: 0.196.. \t\tTest accuracy: 0.943\n",
      "\t\tTrain loss: 0.163.. \t\tTest loss: 0.187.. \t\tTest accuracy: 0.941\n",
      "\t\tTrain loss: 0.169.. \t\tTest loss: 0.157.. \t\tTest accuracy: 0.950\n",
      "\t\tTrain loss: 0.184.. \t\tTest loss: 0.159.. \t\tTest accuracy: 0.952\n",
      "\t\tTrain loss: 0.144.. \t\tTest loss: 0.165.. \t\tTest accuracy: 0.949\n",
      "\t\tTrain loss: 0.155.. \t\tTest loss: 0.172.. \t\tTest accuracy: 0.948\n",
      "\t\tTrain loss: 0.204.. \t\tTest loss: 0.172.. \t\tTest accuracy: 0.946\n",
      "\t\tTrain loss: 0.154.. \t\tTest loss: 0.172.. \t\tTest accuracy: 0.945\n",
      "\t\tTrain loss: 0.159.. \t\tTest loss: 0.156.. \t\tTest accuracy: 0.953\n",
      "\t\tTrain loss: 0.152.. \t\tTest loss: 0.154.. \t\tTest accuracy: 0.950\n",
      "\t\tTrain loss: 0.191.. \t\tTest loss: 0.167.. \t\tTest accuracy: 0.948\n",
      "\t\tTrain loss: 0.175.. \t\tTest loss: 0.165.. \t\tTest accuracy: 0.948\n",
      "\t\tTrain loss: 0.165.. \t\tTest loss: 0.143.. \t\tTest accuracy: 0.958\n",
      "\t\tTrain loss: 0.150.. \t\tTest loss: 0.153.. \t\tTest accuracy: 0.953\n",
      "\t\tTrain loss: 0.167.. \t\tTest loss: 0.148.. \t\tTest accuracy: 0.957\n",
      "\t\tTrain loss: 0.155.. \t\tTest loss: 0.147.. \t\tTest accuracy: 0.958\n",
      "\t\tTrain loss: 0.158.. \t\tTest loss: 0.184.. \t\tTest accuracy: 0.945\n",
      "\t\tTrain loss: 0.171.. \t\tTest loss: 0.177.. \t\tTest accuracy: 0.946\n",
      "\t\tTrain loss: 0.162.. \t\tTest loss: 0.139.. \t\tTest accuracy: 0.957\n",
      "\t\tTrain loss: 0.150.. \t\tTest loss: 0.121.. \t\tTest accuracy: 0.963\n",
      "\t\tTrain loss: 0.135.. \t\tTest loss: 0.146.. \t\tTest accuracy: 0.958\n",
      "\t\tTrain loss: 0.154.. \t\tTest loss: 0.145.. \t\tTest accuracy: 0.956\n",
      "\t\tTrain loss: 0.166.. \t\tTest loss: 0.156.. \t\tTest accuracy: 0.952\n",
      "\t\tTrain loss: 0.124.. \t\tTest loss: 0.161.. \t\tTest accuracy: 0.954\n",
      "\t\tTrain loss: 0.145.. \t\tTest loss: 0.138.. \t\tTest accuracy: 0.957\n",
      "\t\tTrain loss: 0.149.. \t\tTest loss: 0.131.. \t\tTest accuracy: 0.957\n",
      "\t\tTrain loss: 0.194.. \t\tTest loss: 0.159.. \t\tTest accuracy: 0.950\n",
      "\t\tTrain loss: 0.144.. \t\tTest loss: 0.171.. \t\tTest accuracy: 0.947\n",
      "\t\tTrain loss: 0.136.. \t\tTest loss: 0.137.. \t\tTest accuracy: 0.958\n",
      "TRAINING MODEL  2\n",
      "\tEpoch 3/4.. \n",
      "\t\tTrain loss: 0.153.. \t\tTest loss: 0.130.. \t\tTest accuracy: 0.957\n",
      "\t\tTrain loss: 0.115.. \t\tTest loss: 0.149.. \t\tTest accuracy: 0.956\n",
      "\t\tTrain loss: 0.138.. \t\tTest loss: 0.117.. \t\tTest accuracy: 0.967\n",
      "\t\tTrain loss: 0.118.. \t\tTest loss: 0.160.. \t\tTest accuracy: 0.951\n",
      "\t\tTrain loss: 0.151.. \t\tTest loss: 0.125.. \t\tTest accuracy: 0.961\n",
      "\t\tTrain loss: 0.133.. \t\tTest loss: 0.126.. \t\tTest accuracy: 0.964\n",
      "\t\tTrain loss: 0.099.. \t\tTest loss: 0.169.. \t\tTest accuracy: 0.949\n",
      "\t\tTrain loss: 0.128.. \t\tTest loss: 0.142.. \t\tTest accuracy: 0.957\n",
      "\t\tTrain loss: 0.142.. \t\tTest loss: 0.145.. \t\tTest accuracy: 0.956\n",
      "\t\tTrain loss: 0.119.. \t\tTest loss: 0.129.. \t\tTest accuracy: 0.961\n",
      "\t\tTrain loss: 0.109.. \t\tTest loss: 0.151.. \t\tTest accuracy: 0.953\n",
      "\t\tTrain loss: 0.126.. \t\tTest loss: 0.170.. \t\tTest accuracy: 0.948\n",
      "\t\tTrain loss: 0.117.. \t\tTest loss: 0.120.. \t\tTest accuracy: 0.966\n",
      "\t\tTrain loss: 0.138.. \t\tTest loss: 0.160.. \t\tTest accuracy: 0.950\n",
      "\t\tTrain loss: 0.107.. \t\tTest loss: 0.184.. \t\tTest accuracy: 0.942\n",
      "\t\tTrain loss: 0.116.. \t\tTest loss: 0.142.. \t\tTest accuracy: 0.957\n",
      "\t\tTrain loss: 0.143.. \t\tTest loss: 0.120.. \t\tTest accuracy: 0.962\n",
      "\t\tTrain loss: 0.142.. \t\tTest loss: 0.128.. \t\tTest accuracy: 0.962\n",
      "\t\tTrain loss: 0.152.. \t\tTest loss: 0.113.. \t\tTest accuracy: 0.965\n",
      "\t\tTrain loss: 0.127.. \t\tTest loss: 0.126.. \t\tTest accuracy: 0.960\n",
      "\t\tTrain loss: 0.126.. \t\tTest loss: 0.149.. \t\tTest accuracy: 0.953\n",
      "\t\tTrain loss: 0.141.. \t\tTest loss: 0.132.. \t\tTest accuracy: 0.959\n",
      "\t\tTrain loss: 0.183.. \t\tTest loss: 0.131.. \t\tTest accuracy: 0.959\n",
      "\t\tTrain loss: 0.129.. \t\tTest loss: 0.170.. \t\tTest accuracy: 0.948\n",
      "\t\tTrain loss: 0.115.. \t\tTest loss: 0.113.. \t\tTest accuracy: 0.965\n",
      "\t\tTrain loss: 0.121.. \t\tTest loss: 0.122.. \t\tTest accuracy: 0.964\n",
      "\t\tTrain loss: 0.098.. \t\tTest loss: 0.141.. \t\tTest accuracy: 0.959\n",
      "\t\tTrain loss: 0.127.. \t\tTest loss: 0.134.. \t\tTest accuracy: 0.958\n",
      "\t\tTrain loss: 0.116.. \t\tTest loss: 0.122.. \t\tTest accuracy: 0.961\n",
      "\t\tTrain loss: 0.127.. \t\tTest loss: 0.151.. \t\tTest accuracy: 0.953\n",
      "\t\tTrain loss: 0.115.. \t\tTest loss: 0.127.. \t\tTest accuracy: 0.960\n",
      "\t\tTrain loss: 0.100.. \t\tTest loss: 0.111.. \t\tTest accuracy: 0.966\n",
      "\t\tTrain loss: 0.143.. \t\tTest loss: 0.114.. \t\tTest accuracy: 0.966\n",
      "\t\tTrain loss: 0.095.. \t\tTest loss: 0.137.. \t\tTest accuracy: 0.962\n",
      "\t\tTrain loss: 0.122.. \t\tTest loss: 0.155.. \t\tTest accuracy: 0.955\n",
      "\t\tTrain loss: 0.134.. \t\tTest loss: 0.126.. \t\tTest accuracy: 0.962\n",
      "\t\tTrain loss: 0.126.. \t\tTest loss: 0.127.. \t\tTest accuracy: 0.964\n",
      "TRAINING MODEL  2\n",
      "\tEpoch 4/4.. \n",
      "\t\tTrain loss: 0.131.. \t\tTest loss: 0.160.. \t\tTest accuracy: 0.950\n",
      "\t\tTrain loss: 0.102.. \t\tTest loss: 0.144.. \t\tTest accuracy: 0.955\n",
      "\t\tTrain loss: 0.090.. \t\tTest loss: 0.128.. \t\tTest accuracy: 0.963\n",
      "\t\tTrain loss: 0.072.. \t\tTest loss: 0.112.. \t\tTest accuracy: 0.967\n",
      "\t\tTrain loss: 0.121.. \t\tTest loss: 0.118.. \t\tTest accuracy: 0.966\n",
      "\t\tTrain loss: 0.097.. \t\tTest loss: 0.109.. \t\tTest accuracy: 0.968\n",
      "\t\tTrain loss: 0.107.. \t\tTest loss: 0.149.. \t\tTest accuracy: 0.957\n",
      "\t\tTrain loss: 0.083.. \t\tTest loss: 0.132.. \t\tTest accuracy: 0.959\n",
      "\t\tTrain loss: 0.101.. \t\tTest loss: 0.143.. \t\tTest accuracy: 0.955\n",
      "\t\tTrain loss: 0.124.. \t\tTest loss: 0.115.. \t\tTest accuracy: 0.964\n",
      "\t\tTrain loss: 0.141.. \t\tTest loss: 0.128.. \t\tTest accuracy: 0.961\n",
      "\t\tTrain loss: 0.115.. \t\tTest loss: 0.128.. \t\tTest accuracy: 0.960\n",
      "\t\tTrain loss: 0.112.. \t\tTest loss: 0.110.. \t\tTest accuracy: 0.968\n",
      "\t\tTrain loss: 0.125.. \t\tTest loss: 0.118.. \t\tTest accuracy: 0.965\n",
      "\t\tTrain loss: 0.092.. \t\tTest loss: 0.113.. \t\tTest accuracy: 0.966\n",
      "\t\tTrain loss: 0.109.. \t\tTest loss: 0.112.. \t\tTest accuracy: 0.965\n",
      "\t\tTrain loss: 0.120.. \t\tTest loss: 0.116.. \t\tTest accuracy: 0.966\n",
      "\t\tTrain loss: 0.090.. \t\tTest loss: 0.112.. \t\tTest accuracy: 0.967\n",
      "\t\tTrain loss: 0.110.. \t\tTest loss: 0.149.. \t\tTest accuracy: 0.955\n",
      "\t\tTrain loss: 0.095.. \t\tTest loss: 0.119.. \t\tTest accuracy: 0.965\n",
      "\t\tTrain loss: 0.133.. \t\tTest loss: 0.100.. \t\tTest accuracy: 0.970\n",
      "\t\tTrain loss: 0.117.. \t\tTest loss: 0.115.. \t\tTest accuracy: 0.967\n",
      "\t\tTrain loss: 0.077.. \t\tTest loss: 0.105.. \t\tTest accuracy: 0.970\n",
      "\t\tTrain loss: 0.112.. \t\tTest loss: 0.128.. \t\tTest accuracy: 0.963\n",
      "\t\tTrain loss: 0.149.. \t\tTest loss: 0.101.. \t\tTest accuracy: 0.969\n",
      "\t\tTrain loss: 0.078.. \t\tTest loss: 0.107.. \t\tTest accuracy: 0.969\n",
      "\t\tTrain loss: 0.091.. \t\tTest loss: 0.134.. \t\tTest accuracy: 0.961\n",
      "\t\tTrain loss: 0.095.. \t\tTest loss: 0.155.. \t\tTest accuracy: 0.957\n",
      "\t\tTrain loss: 0.106.. \t\tTest loss: 0.111.. \t\tTest accuracy: 0.969\n",
      "\t\tTrain loss: 0.095.. \t\tTest loss: 0.133.. \t\tTest accuracy: 0.960\n",
      "\t\tTrain loss: 0.104.. \t\tTest loss: 0.111.. \t\tTest accuracy: 0.968\n",
      "\t\tTrain loss: 0.112.. \t\tTest loss: 0.126.. \t\tTest accuracy: 0.965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTrain loss: 0.095.. \t\tTest loss: 0.108.. \t\tTest accuracy: 0.967\n",
      "\t\tTrain loss: 0.091.. \t\tTest loss: 0.127.. \t\tTest accuracy: 0.963\n",
      "\t\tTrain loss: 0.129.. \t\tTest loss: 0.122.. \t\tTest accuracy: 0.964\n",
      "\t\tTrain loss: 0.113.. \t\tTest loss: 0.119.. \t\tTest accuracy: 0.964\n",
      "\t\tTrain loss: 0.109.. \t\tTest loss: 0.119.. \t\tTest accuracy: 0.964\n",
      "\t\tTrain loss: 0.111.. \t\tTest loss: 0.109.. \t\tTest accuracy: 0.967\n",
      "TRAINING MODEL  4\n",
      "\tEpoch 1/3.. \n",
      "\t\tTrain loss: 1.627.. \t\tTest loss: 1.050.. \t\tTest accuracy: 0.641\n",
      "\t\tTrain loss: 0.894.. \t\tTest loss: 1.109.. \t\tTest accuracy: 0.684\n",
      "\t\tTrain loss: 0.798.. \t\tTest loss: 0.826.. \t\tTest accuracy: 0.725\n",
      "\t\tTrain loss: 0.710.. \t\tTest loss: 0.998.. \t\tTest accuracy: 0.729\n",
      "\t\tTrain loss: 0.610.. \t\tTest loss: 0.602.. \t\tTest accuracy: 0.819\n",
      "\t\tTrain loss: 0.565.. \t\tTest loss: 0.530.. \t\tTest accuracy: 0.842\n",
      "\t\tTrain loss: 0.603.. \t\tTest loss: 0.595.. \t\tTest accuracy: 0.818\n",
      "\t\tTrain loss: 0.632.. \t\tTest loss: 0.422.. \t\tTest accuracy: 0.880\n",
      "\t\tTrain loss: 0.527.. \t\tTest loss: 0.693.. \t\tTest accuracy: 0.801\n",
      "\t\tTrain loss: 0.515.. \t\tTest loss: 0.490.. \t\tTest accuracy: 0.849\n",
      "\t\tTrain loss: 0.496.. \t\tTest loss: 0.521.. \t\tTest accuracy: 0.865\n",
      "\t\tTrain loss: 0.562.. \t\tTest loss: 0.540.. \t\tTest accuracy: 0.842\n",
      "\t\tTrain loss: 0.508.. \t\tTest loss: 0.523.. \t\tTest accuracy: 0.862\n",
      "\t\tTrain loss: 0.523.. \t\tTest loss: 0.749.. \t\tTest accuracy: 0.798\n",
      "\t\tTrain loss: 0.509.. \t\tTest loss: 0.374.. \t\tTest accuracy: 0.892\n",
      "\t\tTrain loss: 0.522.. \t\tTest loss: 0.413.. \t\tTest accuracy: 0.884\n",
      "\t\tTrain loss: 0.476.. \t\tTest loss: 0.457.. \t\tTest accuracy: 0.874\n",
      "\t\tTrain loss: 0.485.. \t\tTest loss: 0.417.. \t\tTest accuracy: 0.874\n",
      "\t\tTrain loss: 0.467.. \t\tTest loss: 0.426.. \t\tTest accuracy: 0.884\n",
      "\t\tTrain loss: 0.432.. \t\tTest loss: 0.458.. \t\tTest accuracy: 0.873\n",
      "\t\tTrain loss: 0.500.. \t\tTest loss: 0.443.. \t\tTest accuracy: 0.867\n",
      "\t\tTrain loss: 0.545.. \t\tTest loss: 0.687.. \t\tTest accuracy: 0.813\n",
      "\t\tTrain loss: 0.564.. \t\tTest loss: 0.451.. \t\tTest accuracy: 0.860\n",
      "\t\tTrain loss: 0.514.. \t\tTest loss: 0.360.. \t\tTest accuracy: 0.901\n",
      "\t\tTrain loss: 0.458.. \t\tTest loss: 0.377.. \t\tTest accuracy: 0.898\n",
      "\t\tTrain loss: 0.427.. \t\tTest loss: 0.555.. \t\tTest accuracy: 0.856\n",
      "\t\tTrain loss: 0.427.. \t\tTest loss: 0.383.. \t\tTest accuracy: 0.904\n",
      "\t\tTrain loss: 0.471.. \t\tTest loss: 0.428.. \t\tTest accuracy: 0.892\n",
      "\t\tTrain loss: 0.400.. \t\tTest loss: 0.369.. \t\tTest accuracy: 0.903\n",
      "\t\tTrain loss: 0.454.. \t\tTest loss: 0.452.. \t\tTest accuracy: 0.877\n",
      "\t\tTrain loss: 0.377.. \t\tTest loss: 0.443.. \t\tTest accuracy: 0.888\n",
      "\t\tTrain loss: 0.431.. \t\tTest loss: 0.383.. \t\tTest accuracy: 0.906\n",
      "\t\tTrain loss: 0.449.. \t\tTest loss: 0.407.. \t\tTest accuracy: 0.889\n",
      "\t\tTrain loss: 0.394.. \t\tTest loss: 0.412.. \t\tTest accuracy: 0.892\n",
      "\t\tTrain loss: 0.427.. \t\tTest loss: 0.398.. \t\tTest accuracy: 0.891\n",
      "\t\tTrain loss: 0.455.. \t\tTest loss: 0.378.. \t\tTest accuracy: 0.893\n",
      "\t\tTrain loss: 0.449.. \t\tTest loss: 0.410.. \t\tTest accuracy: 0.890\n",
      "TRAINING MODEL  4\n",
      "\tEpoch 2/3.. \n",
      "\t\tTrain loss: 0.400.. \t\tTest loss: 0.391.. \t\tTest accuracy: 0.891\n",
      "\t\tTrain loss: 0.457.. \t\tTest loss: 0.377.. \t\tTest accuracy: 0.890\n",
      "\t\tTrain loss: 0.520.. \t\tTest loss: 0.448.. \t\tTest accuracy: 0.874\n",
      "\t\tTrain loss: 0.447.. \t\tTest loss: 0.404.. \t\tTest accuracy: 0.890\n",
      "\t\tTrain loss: 0.408.. \t\tTest loss: 0.431.. \t\tTest accuracy: 0.876\n",
      "\t\tTrain loss: 0.439.. \t\tTest loss: 0.402.. \t\tTest accuracy: 0.899\n",
      "\t\tTrain loss: 0.379.. \t\tTest loss: 0.394.. \t\tTest accuracy: 0.894\n",
      "\t\tTrain loss: 0.406.. \t\tTest loss: 0.353.. \t\tTest accuracy: 0.906\n",
      "\t\tTrain loss: 0.350.. \t\tTest loss: 0.357.. \t\tTest accuracy: 0.906\n",
      "\t\tTrain loss: 0.380.. \t\tTest loss: 0.333.. \t\tTest accuracy: 0.909\n",
      "\t\tTrain loss: 0.377.. \t\tTest loss: 0.374.. \t\tTest accuracy: 0.905\n",
      "\t\tTrain loss: 0.404.. \t\tTest loss: 0.323.. \t\tTest accuracy: 0.915\n",
      "\t\tTrain loss: 0.415.. \t\tTest loss: 0.381.. \t\tTest accuracy: 0.904\n",
      "\t\tTrain loss: 0.412.. \t\tTest loss: 0.469.. \t\tTest accuracy: 0.877\n",
      "\t\tTrain loss: 0.401.. \t\tTest loss: 0.406.. \t\tTest accuracy: 0.893\n",
      "\t\tTrain loss: 0.348.. \t\tTest loss: 0.367.. \t\tTest accuracy: 0.903\n",
      "\t\tTrain loss: 0.437.. \t\tTest loss: 0.337.. \t\tTest accuracy: 0.912\n",
      "\t\tTrain loss: 0.377.. \t\tTest loss: 0.372.. \t\tTest accuracy: 0.908\n",
      "\t\tTrain loss: 0.410.. \t\tTest loss: 0.359.. \t\tTest accuracy: 0.907\n",
      "\t\tTrain loss: 0.371.. \t\tTest loss: 0.360.. \t\tTest accuracy: 0.903\n",
      "\t\tTrain loss: 0.420.. \t\tTest loss: 0.356.. \t\tTest accuracy: 0.908\n",
      "\t\tTrain loss: 0.316.. \t\tTest loss: 0.338.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.420.. \t\tTest loss: 0.380.. \t\tTest accuracy: 0.901\n",
      "\t\tTrain loss: 0.350.. \t\tTest loss: 0.361.. \t\tTest accuracy: 0.904\n",
      "\t\tTrain loss: 0.388.. \t\tTest loss: 0.370.. \t\tTest accuracy: 0.909\n",
      "\t\tTrain loss: 0.411.. \t\tTest loss: 0.321.. \t\tTest accuracy: 0.915\n",
      "\t\tTrain loss: 0.371.. \t\tTest loss: 0.380.. \t\tTest accuracy: 0.897\n",
      "\t\tTrain loss: 0.352.. \t\tTest loss: 0.342.. \t\tTest accuracy: 0.912\n",
      "\t\tTrain loss: 0.346.. \t\tTest loss: 0.375.. \t\tTest accuracy: 0.902\n",
      "\t\tTrain loss: 0.419.. \t\tTest loss: 0.445.. \t\tTest accuracy: 0.886\n",
      "\t\tTrain loss: 0.380.. \t\tTest loss: 0.404.. \t\tTest accuracy: 0.903\n",
      "\t\tTrain loss: 0.393.. \t\tTest loss: 0.359.. \t\tTest accuracy: 0.904\n",
      "\t\tTrain loss: 0.391.. \t\tTest loss: 0.391.. \t\tTest accuracy: 0.903\n",
      "\t\tTrain loss: 0.487.. \t\tTest loss: 0.484.. \t\tTest accuracy: 0.867\n",
      "\t\tTrain loss: 0.369.. \t\tTest loss: 0.415.. \t\tTest accuracy: 0.892\n",
      "\t\tTrain loss: 0.408.. \t\tTest loss: 0.364.. \t\tTest accuracy: 0.906\n",
      "\t\tTrain loss: 0.381.. \t\tTest loss: 0.304.. \t\tTest accuracy: 0.920\n",
      "\t\tTrain loss: 0.398.. \t\tTest loss: 0.322.. \t\tTest accuracy: 0.919\n",
      "TRAINING MODEL  4\n",
      "\tEpoch 3/3.. \n",
      "\t\tTrain loss: 0.402.. \t\tTest loss: 0.295.. \t\tTest accuracy: 0.922\n",
      "\t\tTrain loss: 0.359.. \t\tTest loss: 0.349.. \t\tTest accuracy: 0.911\n",
      "\t\tTrain loss: 0.371.. \t\tTest loss: 0.343.. \t\tTest accuracy: 0.907\n",
      "\t\tTrain loss: 0.388.. \t\tTest loss: 0.431.. \t\tTest accuracy: 0.897\n",
      "\t\tTrain loss: 0.386.. \t\tTest loss: 0.346.. \t\tTest accuracy: 0.915\n",
      "\t\tTrain loss: 0.382.. \t\tTest loss: 0.318.. \t\tTest accuracy: 0.921\n",
      "\t\tTrain loss: 0.349.. \t\tTest loss: 0.300.. \t\tTest accuracy: 0.922\n",
      "\t\tTrain loss: 0.343.. \t\tTest loss: 0.317.. \t\tTest accuracy: 0.921\n",
      "\t\tTrain loss: 0.346.. \t\tTest loss: 0.380.. \t\tTest accuracy: 0.902\n",
      "\t\tTrain loss: 0.289.. \t\tTest loss: 0.315.. \t\tTest accuracy: 0.921\n",
      "\t\tTrain loss: 0.333.. \t\tTest loss: 0.408.. \t\tTest accuracy: 0.901\n",
      "\t\tTrain loss: 0.387.. \t\tTest loss: 0.353.. \t\tTest accuracy: 0.908\n",
      "\t\tTrain loss: 0.328.. \t\tTest loss: 0.366.. \t\tTest accuracy: 0.918\n",
      "\t\tTrain loss: 0.396.. \t\tTest loss: 0.306.. \t\tTest accuracy: 0.923\n",
      "\t\tTrain loss: 0.381.. \t\tTest loss: 0.367.. \t\tTest accuracy: 0.909\n",
      "\t\tTrain loss: 0.350.. \t\tTest loss: 0.354.. \t\tTest accuracy: 0.907\n",
      "\t\tTrain loss: 0.369.. \t\tTest loss: 0.381.. \t\tTest accuracy: 0.897\n",
      "\t\tTrain loss: 0.332.. \t\tTest loss: 0.371.. \t\tTest accuracy: 0.900\n",
      "\t\tTrain loss: 0.360.. \t\tTest loss: 0.308.. \t\tTest accuracy: 0.922\n",
      "\t\tTrain loss: 0.396.. \t\tTest loss: 0.349.. \t\tTest accuracy: 0.905\n",
      "\t\tTrain loss: 0.354.. \t\tTest loss: 0.329.. \t\tTest accuracy: 0.915\n",
      "\t\tTrain loss: 0.363.. \t\tTest loss: 0.398.. \t\tTest accuracy: 0.896\n",
      "\t\tTrain loss: 0.396.. \t\tTest loss: 0.351.. \t\tTest accuracy: 0.900\n",
      "\t\tTrain loss: 0.397.. \t\tTest loss: 0.354.. \t\tTest accuracy: 0.911\n",
      "\t\tTrain loss: 0.384.. \t\tTest loss: 0.319.. \t\tTest accuracy: 0.918\n",
      "\t\tTrain loss: 0.344.. \t\tTest loss: 0.324.. \t\tTest accuracy: 0.913\n",
      "\t\tTrain loss: 0.408.. \t\tTest loss: 0.333.. \t\tTest accuracy: 0.912\n",
      "\t\tTrain loss: 0.353.. \t\tTest loss: 0.404.. \t\tTest accuracy: 0.895\n",
      "\t\tTrain loss: 0.370.. \t\tTest loss: 0.338.. \t\tTest accuracy: 0.912\n",
      "\t\tTrain loss: 0.444.. \t\tTest loss: 0.384.. \t\tTest accuracy: 0.898\n",
      "\t\tTrain loss: 0.374.. \t\tTest loss: 0.356.. \t\tTest accuracy: 0.918\n",
      "\t\tTrain loss: 0.364.. \t\tTest loss: 0.390.. \t\tTest accuracy: 0.904\n",
      "\t\tTrain loss: 0.383.. \t\tTest loss: 0.352.. \t\tTest accuracy: 0.911\n",
      "\t\tTrain loss: 0.315.. \t\tTest loss: 0.320.. \t\tTest accuracy: 0.919\n",
      "\t\tTrain loss: 0.302.. \t\tTest loss: 0.315.. \t\tTest accuracy: 0.918\n",
      "\t\tTrain loss: 0.346.. \t\tTest loss: 0.299.. \t\tTest accuracy: 0.921\n",
      "\t\tTrain loss: 0.382.. \t\tTest loss: 0.364.. \t\tTest accuracy: 0.897\n",
      "TRAINING MODEL  6\n",
      "\tEpoch 1/2.. \n",
      "\t\tTrain loss: 1.555.. \t\tTest loss: 0.732.. \t\tTest accuracy: 0.778\n",
      "\t\tTrain loss: 0.680.. \t\tTest loss: 0.488.. \t\tTest accuracy: 0.852\n",
      "\t\tTrain loss: 0.565.. \t\tTest loss: 0.545.. \t\tTest accuracy: 0.828\n",
      "\t\tTrain loss: 0.485.. \t\tTest loss: 0.479.. \t\tTest accuracy: 0.854\n",
      "\t\tTrain loss: 0.439.. \t\tTest loss: 0.410.. \t\tTest accuracy: 0.861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTrain loss: 0.368.. \t\tTest loss: 0.356.. \t\tTest accuracy: 0.888\n",
      "\t\tTrain loss: 0.414.. \t\tTest loss: 0.406.. \t\tTest accuracy: 0.870\n",
      "\t\tTrain loss: 0.395.. \t\tTest loss: 0.438.. \t\tTest accuracy: 0.864\n",
      "\t\tTrain loss: 0.356.. \t\tTest loss: 0.327.. \t\tTest accuracy: 0.900\n",
      "\t\tTrain loss: 0.328.. \t\tTest loss: 0.292.. \t\tTest accuracy: 0.910\n",
      "\t\tTrain loss: 0.320.. \t\tTest loss: 0.284.. \t\tTest accuracy: 0.912\n",
      "\t\tTrain loss: 0.286.. \t\tTest loss: 0.259.. \t\tTest accuracy: 0.918\n",
      "\t\tTrain loss: 0.286.. \t\tTest loss: 0.278.. \t\tTest accuracy: 0.911\n",
      "\t\tTrain loss: 0.347.. \t\tTest loss: 0.279.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.287.. \t\tTest loss: 0.260.. \t\tTest accuracy: 0.919\n",
      "\t\tTrain loss: 0.293.. \t\tTest loss: 0.249.. \t\tTest accuracy: 0.922\n",
      "\t\tTrain loss: 0.272.. \t\tTest loss: 0.245.. \t\tTest accuracy: 0.926\n",
      "\t\tTrain loss: 0.270.. \t\tTest loss: 0.225.. \t\tTest accuracy: 0.930\n",
      "\t\tTrain loss: 0.267.. \t\tTest loss: 0.248.. \t\tTest accuracy: 0.919\n",
      "\t\tTrain loss: 0.252.. \t\tTest loss: 0.244.. \t\tTest accuracy: 0.926\n",
      "\t\tTrain loss: 0.243.. \t\tTest loss: 0.275.. \t\tTest accuracy: 0.911\n",
      "\t\tTrain loss: 0.265.. \t\tTest loss: 0.228.. \t\tTest accuracy: 0.930\n",
      "\t\tTrain loss: 0.277.. \t\tTest loss: 0.257.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.224.. \t\tTest loss: 0.192.. \t\tTest accuracy: 0.939\n",
      "\t\tTrain loss: 0.230.. \t\tTest loss: 0.235.. \t\tTest accuracy: 0.925\n",
      "\t\tTrain loss: 0.227.. \t\tTest loss: 0.187.. \t\tTest accuracy: 0.942\n",
      "\t\tTrain loss: 0.212.. \t\tTest loss: 0.180.. \t\tTest accuracy: 0.941\n",
      "\t\tTrain loss: 0.232.. \t\tTest loss: 0.265.. \t\tTest accuracy: 0.918\n",
      "\t\tTrain loss: 0.190.. \t\tTest loss: 0.187.. \t\tTest accuracy: 0.944\n",
      "\t\tTrain loss: 0.214.. \t\tTest loss: 0.189.. \t\tTest accuracy: 0.940\n",
      "\t\tTrain loss: 0.195.. \t\tTest loss: 0.173.. \t\tTest accuracy: 0.946\n",
      "\t\tTrain loss: 0.253.. \t\tTest loss: 0.221.. \t\tTest accuracy: 0.936\n",
      "\t\tTrain loss: 0.201.. \t\tTest loss: 0.206.. \t\tTest accuracy: 0.931\n",
      "\t\tTrain loss: 0.198.. \t\tTest loss: 0.195.. \t\tTest accuracy: 0.941\n",
      "\t\tTrain loss: 0.196.. \t\tTest loss: 0.179.. \t\tTest accuracy: 0.945\n",
      "\t\tTrain loss: 0.202.. \t\tTest loss: 0.163.. \t\tTest accuracy: 0.950\n",
      "\t\tTrain loss: 0.199.. \t\tTest loss: 0.182.. \t\tTest accuracy: 0.942\n",
      "TRAINING MODEL  6\n",
      "\tEpoch 2/2.. \n",
      "\t\tTrain loss: 0.169.. \t\tTest loss: 0.173.. \t\tTest accuracy: 0.948\n",
      "\t\tTrain loss: 0.159.. \t\tTest loss: 0.188.. \t\tTest accuracy: 0.940\n",
      "\t\tTrain loss: 0.198.. \t\tTest loss: 0.175.. \t\tTest accuracy: 0.947\n",
      "\t\tTrain loss: 0.211.. \t\tTest loss: 0.214.. \t\tTest accuracy: 0.932\n",
      "\t\tTrain loss: 0.183.. \t\tTest loss: 0.194.. \t\tTest accuracy: 0.940\n",
      "\t\tTrain loss: 0.208.. \t\tTest loss: 0.149.. \t\tTest accuracy: 0.954\n",
      "\t\tTrain loss: 0.146.. \t\tTest loss: 0.174.. \t\tTest accuracy: 0.948\n",
      "\t\tTrain loss: 0.195.. \t\tTest loss: 0.173.. \t\tTest accuracy: 0.945\n",
      "\t\tTrain loss: 0.176.. \t\tTest loss: 0.163.. \t\tTest accuracy: 0.948\n",
      "\t\tTrain loss: 0.176.. \t\tTest loss: 0.146.. \t\tTest accuracy: 0.956\n",
      "\t\tTrain loss: 0.199.. \t\tTest loss: 0.162.. \t\tTest accuracy: 0.949\n",
      "\t\tTrain loss: 0.147.. \t\tTest loss: 0.152.. \t\tTest accuracy: 0.954\n",
      "\t\tTrain loss: 0.168.. \t\tTest loss: 0.141.. \t\tTest accuracy: 0.958\n",
      "\t\tTrain loss: 0.156.. \t\tTest loss: 0.141.. \t\tTest accuracy: 0.955\n",
      "\t\tTrain loss: 0.177.. \t\tTest loss: 0.171.. \t\tTest accuracy: 0.947\n",
      "\t\tTrain loss: 0.132.. \t\tTest loss: 0.152.. \t\tTest accuracy: 0.953\n",
      "\t\tTrain loss: 0.161.. \t\tTest loss: 0.163.. \t\tTest accuracy: 0.949\n",
      "\t\tTrain loss: 0.131.. \t\tTest loss: 0.149.. \t\tTest accuracy: 0.954\n",
      "\t\tTrain loss: 0.186.. \t\tTest loss: 0.182.. \t\tTest accuracy: 0.942\n",
      "\t\tTrain loss: 0.159.. \t\tTest loss: 0.143.. \t\tTest accuracy: 0.955\n",
      "\t\tTrain loss: 0.163.. \t\tTest loss: 0.170.. \t\tTest accuracy: 0.945\n",
      "\t\tTrain loss: 0.139.. \t\tTest loss: 0.154.. \t\tTest accuracy: 0.951\n",
      "\t\tTrain loss: 0.170.. \t\tTest loss: 0.149.. \t\tTest accuracy: 0.952\n",
      "\t\tTrain loss: 0.142.. \t\tTest loss: 0.182.. \t\tTest accuracy: 0.945\n",
      "\t\tTrain loss: 0.152.. \t\tTest loss: 0.171.. \t\tTest accuracy: 0.948\n",
      "\t\tTrain loss: 0.168.. \t\tTest loss: 0.145.. \t\tTest accuracy: 0.955\n",
      "\t\tTrain loss: 0.154.. \t\tTest loss: 0.144.. \t\tTest accuracy: 0.957\n",
      "\t\tTrain loss: 0.159.. \t\tTest loss: 0.124.. \t\tTest accuracy: 0.961\n",
      "\t\tTrain loss: 0.141.. \t\tTest loss: 0.150.. \t\tTest accuracy: 0.951\n",
      "\t\tTrain loss: 0.133.. \t\tTest loss: 0.154.. \t\tTest accuracy: 0.954\n",
      "\t\tTrain loss: 0.184.. \t\tTest loss: 0.142.. \t\tTest accuracy: 0.956\n",
      "\t\tTrain loss: 0.135.. \t\tTest loss: 0.129.. \t\tTest accuracy: 0.959\n",
      "\t\tTrain loss: 0.139.. \t\tTest loss: 0.131.. \t\tTest accuracy: 0.962\n",
      "\t\tTrain loss: 0.184.. \t\tTest loss: 0.171.. \t\tTest accuracy: 0.948\n",
      "\t\tTrain loss: 0.133.. \t\tTest loss: 0.134.. \t\tTest accuracy: 0.959\n",
      "\t\tTrain loss: 0.120.. \t\tTest loss: 0.115.. \t\tTest accuracy: 0.963\n",
      "\t\tTrain loss: 0.157.. \t\tTest loss: 0.163.. \t\tTest accuracy: 0.953\n",
      "\t\tTrain loss: 0.129.. \t\tTest loss: 0.125.. \t\tTest accuracy: 0.961\n",
      "TRAINING MODEL  7\n",
      "\tEpoch 1/1.. \n",
      "\t\tTrain loss: 1.571.. \t\tTest loss: 1.002.. \t\tTest accuracy: 0.636\n",
      "\t\tTrain loss: 1.006.. \t\tTest loss: 0.732.. \t\tTest accuracy: 0.764\n",
      "\t\tTrain loss: 0.768.. \t\tTest loss: 0.673.. \t\tTest accuracy: 0.787\n",
      "\t\tTrain loss: 0.728.. \t\tTest loss: 0.564.. \t\tTest accuracy: 0.830\n",
      "\t\tTrain loss: 0.659.. \t\tTest loss: 0.637.. \t\tTest accuracy: 0.816\n",
      "\t\tTrain loss: 0.595.. \t\tTest loss: 0.581.. \t\tTest accuracy: 0.848\n",
      "\t\tTrain loss: 0.518.. \t\tTest loss: 0.710.. \t\tTest accuracy: 0.814\n",
      "\t\tTrain loss: 0.620.. \t\tTest loss: 0.731.. \t\tTest accuracy: 0.767\n",
      "\t\tTrain loss: 0.561.. \t\tTest loss: 0.493.. \t\tTest accuracy: 0.872\n",
      "\t\tTrain loss: 0.621.. \t\tTest loss: 0.474.. \t\tTest accuracy: 0.871\n",
      "\t\tTrain loss: 0.600.. \t\tTest loss: 0.474.. \t\tTest accuracy: 0.863\n",
      "\t\tTrain loss: 0.753.. \t\tTest loss: 0.683.. \t\tTest accuracy: 0.789\n",
      "\t\tTrain loss: 0.634.. \t\tTest loss: 0.582.. \t\tTest accuracy: 0.836\n",
      "\t\tTrain loss: 0.585.. \t\tTest loss: 0.597.. \t\tTest accuracy: 0.824\n",
      "\t\tTrain loss: 0.633.. \t\tTest loss: 0.633.. \t\tTest accuracy: 0.822\n",
      "\t\tTrain loss: 0.578.. \t\tTest loss: 0.446.. \t\tTest accuracy: 0.884\n",
      "\t\tTrain loss: 0.564.. \t\tTest loss: 0.414.. \t\tTest accuracy: 0.878\n",
      "\t\tTrain loss: 0.490.. \t\tTest loss: 0.460.. \t\tTest accuracy: 0.864\n",
      "\t\tTrain loss: 0.585.. \t\tTest loss: 0.387.. \t\tTest accuracy: 0.898\n",
      "\t\tTrain loss: 0.536.. \t\tTest loss: 0.480.. \t\tTest accuracy: 0.865\n",
      "\t\tTrain loss: 0.464.. \t\tTest loss: 0.465.. \t\tTest accuracy: 0.871\n",
      "\t\tTrain loss: 0.569.. \t\tTest loss: 0.553.. \t\tTest accuracy: 0.820\n",
      "\t\tTrain loss: 0.669.. \t\tTest loss: 0.463.. \t\tTest accuracy: 0.874\n",
      "\t\tTrain loss: 0.502.. \t\tTest loss: 0.588.. \t\tTest accuracy: 0.844\n",
      "\t\tTrain loss: 0.491.. \t\tTest loss: 0.441.. \t\tTest accuracy: 0.879\n",
      "\t\tTrain loss: 0.387.. \t\tTest loss: 0.397.. \t\tTest accuracy: 0.894\n",
      "\t\tTrain loss: 0.440.. \t\tTest loss: 0.468.. \t\tTest accuracy: 0.867\n",
      "\t\tTrain loss: 0.427.. \t\tTest loss: 0.334.. \t\tTest accuracy: 0.907\n",
      "\t\tTrain loss: 0.415.. \t\tTest loss: 0.421.. \t\tTest accuracy: 0.886\n",
      "\t\tTrain loss: 0.479.. \t\tTest loss: 0.406.. \t\tTest accuracy: 0.880\n",
      "\t\tTrain loss: 0.441.. \t\tTest loss: 0.411.. \t\tTest accuracy: 0.897\n",
      "\t\tTrain loss: 0.486.. \t\tTest loss: 0.401.. \t\tTest accuracy: 0.885\n",
      "\t\tTrain loss: 0.430.. \t\tTest loss: 0.422.. \t\tTest accuracy: 0.882\n",
      "\t\tTrain loss: 0.406.. \t\tTest loss: 0.367.. \t\tTest accuracy: 0.899\n",
      "\t\tTrain loss: 0.376.. \t\tTest loss: 0.573.. \t\tTest accuracy: 0.852\n",
      "\t\tTrain loss: 0.535.. \t\tTest loss: 0.497.. \t\tTest accuracy: 0.861\n",
      "\t\tTrain loss: 0.417.. \t\tTest loss: 0.410.. \t\tTest accuracy: 0.897\n",
      "TRAINING MODEL  8\n",
      "\tEpoch 1/3.. \n",
      "\t\tTrain loss: 2.188.. \t\tTest loss: 2.000.. \t\tTest accuracy: 0.519\n",
      "\t\tTrain loss: 1.687.. \t\tTest loss: 1.348.. \t\tTest accuracy: 0.676\n",
      "\t\tTrain loss: 1.097.. \t\tTest loss: 0.896.. \t\tTest accuracy: 0.776\n",
      "\t\tTrain loss: 0.835.. \t\tTest loss: 0.683.. \t\tTest accuracy: 0.834\n",
      "\t\tTrain loss: 0.673.. \t\tTest loss: 0.590.. \t\tTest accuracy: 0.847\n",
      "\t\tTrain loss: 0.569.. \t\tTest loss: 0.538.. \t\tTest accuracy: 0.851\n",
      "\t\tTrain loss: 0.531.. \t\tTest loss: 0.486.. \t\tTest accuracy: 0.868\n",
      "\t\tTrain loss: 0.485.. \t\tTest loss: 0.467.. \t\tTest accuracy: 0.869\n",
      "\t\tTrain loss: 0.474.. \t\tTest loss: 0.433.. \t\tTest accuracy: 0.875\n",
      "\t\tTrain loss: 0.464.. \t\tTest loss: 0.435.. \t\tTest accuracy: 0.873\n",
      "\t\tTrain loss: 0.430.. \t\tTest loss: 0.396.. \t\tTest accuracy: 0.882\n",
      "\t\tTrain loss: 0.390.. \t\tTest loss: 0.401.. \t\tTest accuracy: 0.883\n",
      "\t\tTrain loss: 0.384.. \t\tTest loss: 0.394.. \t\tTest accuracy: 0.885\n",
      "\t\tTrain loss: 0.370.. \t\tTest loss: 0.366.. \t\tTest accuracy: 0.897\n",
      "\t\tTrain loss: 0.373.. \t\tTest loss: 0.381.. \t\tTest accuracy: 0.893\n",
      "\t\tTrain loss: 0.353.. \t\tTest loss: 0.345.. \t\tTest accuracy: 0.896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTrain loss: 0.375.. \t\tTest loss: 0.352.. \t\tTest accuracy: 0.897\n",
      "\t\tTrain loss: 0.373.. \t\tTest loss: 0.349.. \t\tTest accuracy: 0.899\n",
      "\t\tTrain loss: 0.370.. \t\tTest loss: 0.356.. \t\tTest accuracy: 0.894\n",
      "\t\tTrain loss: 0.355.. \t\tTest loss: 0.335.. \t\tTest accuracy: 0.898\n",
      "\t\tTrain loss: 0.356.. \t\tTest loss: 0.321.. \t\tTest accuracy: 0.904\n",
      "\t\tTrain loss: 0.363.. \t\tTest loss: 0.321.. \t\tTest accuracy: 0.904\n",
      "\t\tTrain loss: 0.334.. \t\tTest loss: 0.330.. \t\tTest accuracy: 0.899\n",
      "\t\tTrain loss: 0.333.. \t\tTest loss: 0.312.. \t\tTest accuracy: 0.906\n",
      "\t\tTrain loss: 0.333.. \t\tTest loss: 0.328.. \t\tTest accuracy: 0.899\n",
      "\t\tTrain loss: 0.331.. \t\tTest loss: 0.320.. \t\tTest accuracy: 0.899\n",
      "\t\tTrain loss: 0.397.. \t\tTest loss: 0.314.. \t\tTest accuracy: 0.904\n",
      "\t\tTrain loss: 0.350.. \t\tTest loss: 0.299.. \t\tTest accuracy: 0.909\n",
      "\t\tTrain loss: 0.298.. \t\tTest loss: 0.295.. \t\tTest accuracy: 0.912\n",
      "\t\tTrain loss: 0.307.. \t\tTest loss: 0.302.. \t\tTest accuracy: 0.909\n",
      "\t\tTrain loss: 0.295.. \t\tTest loss: 0.290.. \t\tTest accuracy: 0.913\n",
      "\t\tTrain loss: 0.348.. \t\tTest loss: 0.289.. \t\tTest accuracy: 0.917\n",
      "\t\tTrain loss: 0.330.. \t\tTest loss: 0.288.. \t\tTest accuracy: 0.911\n",
      "\t\tTrain loss: 0.285.. \t\tTest loss: 0.287.. \t\tTest accuracy: 0.913\n",
      "\t\tTrain loss: 0.317.. \t\tTest loss: 0.280.. \t\tTest accuracy: 0.917\n",
      "\t\tTrain loss: 0.304.. \t\tTest loss: 0.282.. \t\tTest accuracy: 0.915\n",
      "\t\tTrain loss: 0.317.. \t\tTest loss: 0.276.. \t\tTest accuracy: 0.918\n",
      "TRAINING MODEL  8\n",
      "\tEpoch 2/3.. \n",
      "\t\tTrain loss: 0.306.. \t\tTest loss: 0.290.. \t\tTest accuracy: 0.912\n",
      "\t\tTrain loss: 0.310.. \t\tTest loss: 0.270.. \t\tTest accuracy: 0.920\n",
      "\t\tTrain loss: 0.299.. \t\tTest loss: 0.261.. \t\tTest accuracy: 0.924\n",
      "\t\tTrain loss: 0.247.. \t\tTest loss: 0.272.. \t\tTest accuracy: 0.919\n",
      "\t\tTrain loss: 0.283.. \t\tTest loss: 0.258.. \t\tTest accuracy: 0.922\n",
      "\t\tTrain loss: 0.239.. \t\tTest loss: 0.261.. \t\tTest accuracy: 0.923\n",
      "\t\tTrain loss: 0.255.. \t\tTest loss: 0.255.. \t\tTest accuracy: 0.922\n",
      "\t\tTrain loss: 0.303.. \t\tTest loss: 0.248.. \t\tTest accuracy: 0.925\n",
      "\t\tTrain loss: 0.247.. \t\tTest loss: 0.249.. \t\tTest accuracy: 0.926\n",
      "\t\tTrain loss: 0.292.. \t\tTest loss: 0.244.. \t\tTest accuracy: 0.928\n",
      "\t\tTrain loss: 0.276.. \t\tTest loss: 0.266.. \t\tTest accuracy: 0.920\n",
      "\t\tTrain loss: 0.241.. \t\tTest loss: 0.252.. \t\tTest accuracy: 0.925\n",
      "\t\tTrain loss: 0.256.. \t\tTest loss: 0.238.. \t\tTest accuracy: 0.931\n",
      "\t\tTrain loss: 0.235.. \t\tTest loss: 0.247.. \t\tTest accuracy: 0.926\n",
      "\t\tTrain loss: 0.225.. \t\tTest loss: 0.241.. \t\tTest accuracy: 0.927\n",
      "\t\tTrain loss: 0.272.. \t\tTest loss: 0.231.. \t\tTest accuracy: 0.930\n",
      "\t\tTrain loss: 0.245.. \t\tTest loss: 0.226.. \t\tTest accuracy: 0.932\n",
      "\t\tTrain loss: 0.224.. \t\tTest loss: 0.225.. \t\tTest accuracy: 0.931\n",
      "\t\tTrain loss: 0.192.. \t\tTest loss: 0.221.. \t\tTest accuracy: 0.933\n",
      "\t\tTrain loss: 0.241.. \t\tTest loss: 0.243.. \t\tTest accuracy: 0.926\n",
      "\t\tTrain loss: 0.232.. \t\tTest loss: 0.227.. \t\tTest accuracy: 0.929\n",
      "\t\tTrain loss: 0.231.. \t\tTest loss: 0.227.. \t\tTest accuracy: 0.929\n",
      "\t\tTrain loss: 0.227.. \t\tTest loss: 0.227.. \t\tTest accuracy: 0.931\n",
      "\t\tTrain loss: 0.261.. \t\tTest loss: 0.228.. \t\tTest accuracy: 0.935\n",
      "\t\tTrain loss: 0.238.. \t\tTest loss: 0.217.. \t\tTest accuracy: 0.933\n",
      "\t\tTrain loss: 0.190.. \t\tTest loss: 0.244.. \t\tTest accuracy: 0.924\n",
      "\t\tTrain loss: 0.252.. \t\tTest loss: 0.204.. \t\tTest accuracy: 0.940\n",
      "\t\tTrain loss: 0.241.. \t\tTest loss: 0.234.. \t\tTest accuracy: 0.932\n",
      "\t\tTrain loss: 0.203.. \t\tTest loss: 0.207.. \t\tTest accuracy: 0.939\n",
      "\t\tTrain loss: 0.217.. \t\tTest loss: 0.207.. \t\tTest accuracy: 0.938\n",
      "\t\tTrain loss: 0.210.. \t\tTest loss: 0.230.. \t\tTest accuracy: 0.930\n",
      "\t\tTrain loss: 0.205.. \t\tTest loss: 0.208.. \t\tTest accuracy: 0.936\n",
      "\t\tTrain loss: 0.197.. \t\tTest loss: 0.210.. \t\tTest accuracy: 0.938\n",
      "\t\tTrain loss: 0.264.. \t\tTest loss: 0.209.. \t\tTest accuracy: 0.937\n",
      "\t\tTrain loss: 0.222.. \t\tTest loss: 0.215.. \t\tTest accuracy: 0.933\n",
      "\t\tTrain loss: 0.211.. \t\tTest loss: 0.193.. \t\tTest accuracy: 0.941\n",
      "\t\tTrain loss: 0.195.. \t\tTest loss: 0.198.. \t\tTest accuracy: 0.941\n",
      "\t\tTrain loss: 0.217.. \t\tTest loss: 0.195.. \t\tTest accuracy: 0.940\n",
      "TRAINING MODEL  8\n",
      "\tEpoch 3/3.. \n",
      "\t\tTrain loss: 0.183.. \t\tTest loss: 0.189.. \t\tTest accuracy: 0.943\n",
      "\t\tTrain loss: 0.205.. \t\tTest loss: 0.192.. \t\tTest accuracy: 0.944\n",
      "\t\tTrain loss: 0.175.. \t\tTest loss: 0.189.. \t\tTest accuracy: 0.941\n",
      "\t\tTrain loss: 0.182.. \t\tTest loss: 0.190.. \t\tTest accuracy: 0.940\n",
      "\t\tTrain loss: 0.212.. \t\tTest loss: 0.182.. \t\tTest accuracy: 0.946\n",
      "\t\tTrain loss: 0.233.. \t\tTest loss: 0.193.. \t\tTest accuracy: 0.939\n",
      "\t\tTrain loss: 0.169.. \t\tTest loss: 0.194.. \t\tTest accuracy: 0.939\n",
      "\t\tTrain loss: 0.210.. \t\tTest loss: 0.178.. \t\tTest accuracy: 0.946\n",
      "\t\tTrain loss: 0.178.. \t\tTest loss: 0.179.. \t\tTest accuracy: 0.944\n",
      "\t\tTrain loss: 0.190.. \t\tTest loss: 0.181.. \t\tTest accuracy: 0.945\n",
      "\t\tTrain loss: 0.204.. \t\tTest loss: 0.188.. \t\tTest accuracy: 0.944\n",
      "\t\tTrain loss: 0.169.. \t\tTest loss: 0.172.. \t\tTest accuracy: 0.948\n",
      "\t\tTrain loss: 0.180.. \t\tTest loss: 0.184.. \t\tTest accuracy: 0.942\n",
      "\t\tTrain loss: 0.144.. \t\tTest loss: 0.173.. \t\tTest accuracy: 0.946\n",
      "\t\tTrain loss: 0.180.. \t\tTest loss: 0.188.. \t\tTest accuracy: 0.944\n",
      "\t\tTrain loss: 0.180.. \t\tTest loss: 0.175.. \t\tTest accuracy: 0.945\n",
      "\t\tTrain loss: 0.180.. \t\tTest loss: 0.168.. \t\tTest accuracy: 0.948\n",
      "\t\tTrain loss: 0.172.. \t\tTest loss: 0.180.. \t\tTest accuracy: 0.944\n",
      "\t\tTrain loss: 0.152.. \t\tTest loss: 0.168.. \t\tTest accuracy: 0.950\n",
      "\t\tTrain loss: 0.166.. \t\tTest loss: 0.181.. \t\tTest accuracy: 0.945\n",
      "\t\tTrain loss: 0.194.. \t\tTest loss: 0.173.. \t\tTest accuracy: 0.949\n",
      "\t\tTrain loss: 0.178.. \t\tTest loss: 0.169.. \t\tTest accuracy: 0.951\n",
      "\t\tTrain loss: 0.160.. \t\tTest loss: 0.168.. \t\tTest accuracy: 0.951\n",
      "\t\tTrain loss: 0.170.. \t\tTest loss: 0.169.. \t\tTest accuracy: 0.948\n",
      "\t\tTrain loss: 0.164.. \t\tTest loss: 0.155.. \t\tTest accuracy: 0.953\n",
      "\t\tTrain loss: 0.186.. \t\tTest loss: 0.164.. \t\tTest accuracy: 0.950\n",
      "\t\tTrain loss: 0.171.. \t\tTest loss: 0.168.. \t\tTest accuracy: 0.950\n",
      "\t\tTrain loss: 0.155.. \t\tTest loss: 0.170.. \t\tTest accuracy: 0.946\n",
      "\t\tTrain loss: 0.157.. \t\tTest loss: 0.160.. \t\tTest accuracy: 0.951\n",
      "\t\tTrain loss: 0.163.. \t\tTest loss: 0.152.. \t\tTest accuracy: 0.953\n",
      "\t\tTrain loss: 0.156.. \t\tTest loss: 0.158.. \t\tTest accuracy: 0.951\n",
      "\t\tTrain loss: 0.195.. \t\tTest loss: 0.155.. \t\tTest accuracy: 0.953\n",
      "\t\tTrain loss: 0.166.. \t\tTest loss: 0.148.. \t\tTest accuracy: 0.955\n",
      "\t\tTrain loss: 0.143.. \t\tTest loss: 0.151.. \t\tTest accuracy: 0.955\n",
      "\t\tTrain loss: 0.156.. \t\tTest loss: 0.153.. \t\tTest accuracy: 0.953\n",
      "\t\tTrain loss: 0.170.. \t\tTest loss: 0.149.. \t\tTest accuracy: 0.955\n",
      "\t\tTrain loss: 0.167.. \t\tTest loss: 0.153.. \t\tTest accuracy: 0.954\n",
      "TRAINING MODEL  9\n",
      "\tEpoch 1/4.. \n",
      "\t\tTrain loss: 1.463.. \t\tTest loss: 0.832.. \t\tTest accuracy: 0.712\n",
      "\t\tTrain loss: 0.780.. \t\tTest loss: 0.730.. \t\tTest accuracy: 0.775\n",
      "\t\tTrain loss: 0.683.. \t\tTest loss: 0.630.. \t\tTest accuracy: 0.795\n",
      "\t\tTrain loss: 0.683.. \t\tTest loss: 1.006.. \t\tTest accuracy: 0.677\n",
      "\t\tTrain loss: 0.670.. \t\tTest loss: 0.498.. \t\tTest accuracy: 0.857\n",
      "\t\tTrain loss: 0.604.. \t\tTest loss: 0.548.. \t\tTest accuracy: 0.823\n",
      "\t\tTrain loss: 0.493.. \t\tTest loss: 0.529.. \t\tTest accuracy: 0.854\n",
      "\t\tTrain loss: 0.497.. \t\tTest loss: 0.437.. \t\tTest accuracy: 0.876\n",
      "\t\tTrain loss: 0.525.. \t\tTest loss: 0.450.. \t\tTest accuracy: 0.867\n",
      "\t\tTrain loss: 0.461.. \t\tTest loss: 0.513.. \t\tTest accuracy: 0.854\n",
      "\t\tTrain loss: 0.635.. \t\tTest loss: 0.601.. \t\tTest accuracy: 0.828\n",
      "\t\tTrain loss: 0.561.. \t\tTest loss: 0.479.. \t\tTest accuracy: 0.868\n",
      "\t\tTrain loss: 0.602.. \t\tTest loss: 0.465.. \t\tTest accuracy: 0.863\n",
      "\t\tTrain loss: 0.511.. \t\tTest loss: 0.467.. \t\tTest accuracy: 0.870\n",
      "\t\tTrain loss: 0.510.. \t\tTest loss: 0.422.. \t\tTest accuracy: 0.879\n",
      "\t\tTrain loss: 0.456.. \t\tTest loss: 0.377.. \t\tTest accuracy: 0.896\n",
      "\t\tTrain loss: 0.466.. \t\tTest loss: 0.458.. \t\tTest accuracy: 0.884\n",
      "\t\tTrain loss: 0.510.. \t\tTest loss: 0.493.. \t\tTest accuracy: 0.856\n",
      "\t\tTrain loss: 0.448.. \t\tTest loss: 0.489.. \t\tTest accuracy: 0.880\n",
      "\t\tTrain loss: 0.464.. \t\tTest loss: 0.421.. \t\tTest accuracy: 0.883\n",
      "\t\tTrain loss: 0.479.. \t\tTest loss: 0.451.. \t\tTest accuracy: 0.880\n",
      "\t\tTrain loss: 0.507.. \t\tTest loss: 0.449.. \t\tTest accuracy: 0.888\n",
      "\t\tTrain loss: 0.444.. \t\tTest loss: 0.378.. \t\tTest accuracy: 0.897\n",
      "\t\tTrain loss: 0.413.. \t\tTest loss: 0.420.. \t\tTest accuracy: 0.886\n",
      "\t\tTrain loss: 0.474.. \t\tTest loss: 0.364.. \t\tTest accuracy: 0.898\n",
      "\t\tTrain loss: 0.482.. \t\tTest loss: 0.403.. \t\tTest accuracy: 0.896\n",
      "\t\tTrain loss: 0.406.. \t\tTest loss: 0.374.. \t\tTest accuracy: 0.904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTrain loss: 0.542.. \t\tTest loss: 0.663.. \t\tTest accuracy: 0.807\n",
      "\t\tTrain loss: 0.386.. \t\tTest loss: 0.428.. \t\tTest accuracy: 0.888\n",
      "\t\tTrain loss: 0.465.. \t\tTest loss: 0.318.. \t\tTest accuracy: 0.911\n",
      "\t\tTrain loss: 0.373.. \t\tTest loss: 0.467.. \t\tTest accuracy: 0.884\n",
      "\t\tTrain loss: 0.422.. \t\tTest loss: 0.346.. \t\tTest accuracy: 0.902\n",
      "\t\tTrain loss: 0.426.. \t\tTest loss: 0.349.. \t\tTest accuracy: 0.902\n",
      "\t\tTrain loss: 0.401.. \t\tTest loss: 0.354.. \t\tTest accuracy: 0.900\n",
      "\t\tTrain loss: 0.454.. \t\tTest loss: 0.453.. \t\tTest accuracy: 0.882\n",
      "\t\tTrain loss: 0.462.. \t\tTest loss: 0.589.. \t\tTest accuracy: 0.845\n",
      "\t\tTrain loss: 0.480.. \t\tTest loss: 0.406.. \t\tTest accuracy: 0.891\n",
      "TRAINING MODEL  9\n",
      "\tEpoch 2/4.. \n",
      "\t\tTrain loss: 0.438.. \t\tTest loss: 0.370.. \t\tTest accuracy: 0.903\n",
      "\t\tTrain loss: 0.375.. \t\tTest loss: 0.370.. \t\tTest accuracy: 0.898\n",
      "\t\tTrain loss: 0.418.. \t\tTest loss: 0.407.. \t\tTest accuracy: 0.897\n",
      "\t\tTrain loss: 0.439.. \t\tTest loss: 0.838.. \t\tTest accuracy: 0.792\n",
      "\t\tTrain loss: 0.457.. \t\tTest loss: 0.373.. \t\tTest accuracy: 0.896\n",
      "\t\tTrain loss: 0.368.. \t\tTest loss: 0.492.. \t\tTest accuracy: 0.883\n",
      "\t\tTrain loss: 0.464.. \t\tTest loss: 0.557.. \t\tTest accuracy: 0.867\n",
      "\t\tTrain loss: 0.477.. \t\tTest loss: 0.408.. \t\tTest accuracy: 0.889\n",
      "\t\tTrain loss: 0.381.. \t\tTest loss: 0.349.. \t\tTest accuracy: 0.910\n",
      "\t\tTrain loss: 0.385.. \t\tTest loss: 0.548.. \t\tTest accuracy: 0.862\n",
      "\t\tTrain loss: 0.364.. \t\tTest loss: 0.318.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.346.. \t\tTest loss: 0.375.. \t\tTest accuracy: 0.900\n",
      "\t\tTrain loss: 0.392.. \t\tTest loss: 0.345.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.397.. \t\tTest loss: 0.347.. \t\tTest accuracy: 0.902\n",
      "\t\tTrain loss: 0.346.. \t\tTest loss: 0.387.. \t\tTest accuracy: 0.909\n",
      "\t\tTrain loss: 0.401.. \t\tTest loss: 0.312.. \t\tTest accuracy: 0.920\n",
      "\t\tTrain loss: 0.361.. \t\tTest loss: 0.441.. \t\tTest accuracy: 0.896\n",
      "\t\tTrain loss: 0.516.. \t\tTest loss: 0.365.. \t\tTest accuracy: 0.903\n",
      "\t\tTrain loss: 0.408.. \t\tTest loss: 0.338.. \t\tTest accuracy: 0.909\n",
      "\t\tTrain loss: 0.371.. \t\tTest loss: 0.335.. \t\tTest accuracy: 0.908\n",
      "\t\tTrain loss: 0.435.. \t\tTest loss: 0.533.. \t\tTest accuracy: 0.857\n",
      "\t\tTrain loss: 0.410.. \t\tTest loss: 0.380.. \t\tTest accuracy: 0.899\n",
      "\t\tTrain loss: 0.450.. \t\tTest loss: 0.435.. \t\tTest accuracy: 0.887\n",
      "\t\tTrain loss: 0.370.. \t\tTest loss: 0.336.. \t\tTest accuracy: 0.908\n",
      "\t\tTrain loss: 0.420.. \t\tTest loss: 0.352.. \t\tTest accuracy: 0.904\n",
      "\t\tTrain loss: 0.390.. \t\tTest loss: 0.347.. \t\tTest accuracy: 0.909\n",
      "\t\tTrain loss: 0.415.. \t\tTest loss: 0.467.. \t\tTest accuracy: 0.868\n",
      "\t\tTrain loss: 0.364.. \t\tTest loss: 0.355.. \t\tTest accuracy: 0.912\n",
      "\t\tTrain loss: 0.373.. \t\tTest loss: 0.374.. \t\tTest accuracy: 0.906\n",
      "\t\tTrain loss: 0.392.. \t\tTest loss: 0.386.. \t\tTest accuracy: 0.907\n",
      "\t\tTrain loss: 0.397.. \t\tTest loss: 0.442.. \t\tTest accuracy: 0.880\n",
      "\t\tTrain loss: 0.377.. \t\tTest loss: 0.336.. \t\tTest accuracy: 0.909\n",
      "\t\tTrain loss: 0.311.. \t\tTest loss: 0.348.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.365.. \t\tTest loss: 0.388.. \t\tTest accuracy: 0.904\n",
      "\t\tTrain loss: 0.361.. \t\tTest loss: 0.396.. \t\tTest accuracy: 0.884\n",
      "\t\tTrain loss: 0.409.. \t\tTest loss: 0.474.. \t\tTest accuracy: 0.875\n",
      "\t\tTrain loss: 0.442.. \t\tTest loss: 0.329.. \t\tTest accuracy: 0.920\n",
      "\t\tTrain loss: 0.362.. \t\tTest loss: 0.326.. \t\tTest accuracy: 0.916\n",
      "TRAINING MODEL  9\n",
      "\tEpoch 3/4.. \n",
      "\t\tTrain loss: 0.499.. \t\tTest loss: 0.427.. \t\tTest accuracy: 0.881\n",
      "\t\tTrain loss: 0.454.. \t\tTest loss: 0.332.. \t\tTest accuracy: 0.912\n",
      "\t\tTrain loss: 0.315.. \t\tTest loss: 0.346.. \t\tTest accuracy: 0.913\n",
      "\t\tTrain loss: 0.370.. \t\tTest loss: 0.339.. \t\tTest accuracy: 0.909\n",
      "\t\tTrain loss: 0.359.. \t\tTest loss: 0.434.. \t\tTest accuracy: 0.892\n",
      "\t\tTrain loss: 0.442.. \t\tTest loss: 0.351.. \t\tTest accuracy: 0.910\n",
      "\t\tTrain loss: 0.406.. \t\tTest loss: 0.474.. \t\tTest accuracy: 0.886\n",
      "\t\tTrain loss: 0.431.. \t\tTest loss: 0.471.. \t\tTest accuracy: 0.891\n",
      "\t\tTrain loss: 0.445.. \t\tTest loss: 0.410.. \t\tTest accuracy: 0.903\n",
      "\t\tTrain loss: 0.403.. \t\tTest loss: 0.344.. \t\tTest accuracy: 0.914\n",
      "\t\tTrain loss: 0.389.. \t\tTest loss: 0.343.. \t\tTest accuracy: 0.913\n",
      "\t\tTrain loss: 0.367.. \t\tTest loss: 0.341.. \t\tTest accuracy: 0.914\n",
      "\t\tTrain loss: 0.442.. \t\tTest loss: 0.610.. \t\tTest accuracy: 0.817\n",
      "\t\tTrain loss: 0.484.. \t\tTest loss: 0.371.. \t\tTest accuracy: 0.911\n",
      "\t\tTrain loss: 0.325.. \t\tTest loss: 0.397.. \t\tTest accuracy: 0.900\n",
      "\t\tTrain loss: 0.406.. \t\tTest loss: 0.392.. \t\tTest accuracy: 0.899\n",
      "\t\tTrain loss: 0.374.. \t\tTest loss: 0.294.. \t\tTest accuracy: 0.923\n",
      "\t\tTrain loss: 0.384.. \t\tTest loss: 0.374.. \t\tTest accuracy: 0.908\n",
      "\t\tTrain loss: 0.392.. \t\tTest loss: 0.350.. \t\tTest accuracy: 0.909\n",
      "\t\tTrain loss: 0.320.. \t\tTest loss: 0.336.. \t\tTest accuracy: 0.914\n",
      "\t\tTrain loss: 0.380.. \t\tTest loss: 0.313.. \t\tTest accuracy: 0.919\n",
      "\t\tTrain loss: 0.339.. \t\tTest loss: 0.393.. \t\tTest accuracy: 0.897\n",
      "\t\tTrain loss: 0.342.. \t\tTest loss: 0.318.. \t\tTest accuracy: 0.915\n",
      "\t\tTrain loss: 0.355.. \t\tTest loss: 0.302.. \t\tTest accuracy: 0.924\n",
      "\t\tTrain loss: 0.333.. \t\tTest loss: 0.389.. \t\tTest accuracy: 0.904\n",
      "\t\tTrain loss: 0.347.. \t\tTest loss: 0.315.. \t\tTest accuracy: 0.923\n",
      "\t\tTrain loss: 0.334.. \t\tTest loss: 0.335.. \t\tTest accuracy: 0.910\n",
      "\t\tTrain loss: 0.375.. \t\tTest loss: 0.299.. \t\tTest accuracy: 0.925\n",
      "\t\tTrain loss: 0.362.. \t\tTest loss: 0.315.. \t\tTest accuracy: 0.921\n",
      "\t\tTrain loss: 0.321.. \t\tTest loss: 0.328.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.414.. \t\tTest loss: 0.422.. \t\tTest accuracy: 0.901\n",
      "\t\tTrain loss: 0.322.. \t\tTest loss: 0.310.. \t\tTest accuracy: 0.922\n",
      "\t\tTrain loss: 0.432.. \t\tTest loss: 0.378.. \t\tTest accuracy: 0.914\n",
      "\t\tTrain loss: 0.394.. \t\tTest loss: 0.358.. \t\tTest accuracy: 0.910\n",
      "\t\tTrain loss: 0.328.. \t\tTest loss: 0.344.. \t\tTest accuracy: 0.921\n",
      "\t\tTrain loss: 0.360.. \t\tTest loss: 0.337.. \t\tTest accuracy: 0.915\n",
      "\t\tTrain loss: 0.377.. \t\tTest loss: 0.336.. \t\tTest accuracy: 0.915\n",
      "TRAINING MODEL  9\n",
      "\tEpoch 4/4.. \n",
      "\t\tTrain loss: 0.407.. \t\tTest loss: 0.372.. \t\tTest accuracy: 0.906\n",
      "\t\tTrain loss: 0.364.. \t\tTest loss: 0.376.. \t\tTest accuracy: 0.902\n",
      "\t\tTrain loss: 0.369.. \t\tTest loss: 0.454.. \t\tTest accuracy: 0.893\n",
      "\t\tTrain loss: 0.410.. \t\tTest loss: 0.346.. \t\tTest accuracy: 0.906\n",
      "\t\tTrain loss: 0.352.. \t\tTest loss: 0.320.. \t\tTest accuracy: 0.917\n",
      "\t\tTrain loss: 0.276.. \t\tTest loss: 0.349.. \t\tTest accuracy: 0.911\n",
      "\t\tTrain loss: 0.403.. \t\tTest loss: 0.395.. \t\tTest accuracy: 0.898\n",
      "\t\tTrain loss: 0.344.. \t\tTest loss: 0.292.. \t\tTest accuracy: 0.928\n",
      "\t\tTrain loss: 0.282.. \t\tTest loss: 0.295.. \t\tTest accuracy: 0.929\n",
      "\t\tTrain loss: 0.620.. \t\tTest loss: 0.838.. \t\tTest accuracy: 0.683\n",
      "\t\tTrain loss: 0.502.. \t\tTest loss: 0.611.. \t\tTest accuracy: 0.842\n",
      "\t\tTrain loss: 0.472.. \t\tTest loss: 0.378.. \t\tTest accuracy: 0.905\n",
      "\t\tTrain loss: 0.363.. \t\tTest loss: 0.398.. \t\tTest accuracy: 0.903\n",
      "\t\tTrain loss: 0.414.. \t\tTest loss: 0.377.. \t\tTest accuracy: 0.910\n",
      "\t\tTrain loss: 0.342.. \t\tTest loss: 0.356.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.338.. \t\tTest loss: 0.318.. \t\tTest accuracy: 0.927\n",
      "\t\tTrain loss: 0.376.. \t\tTest loss: 0.310.. \t\tTest accuracy: 0.925\n",
      "\t\tTrain loss: 0.353.. \t\tTest loss: 0.306.. \t\tTest accuracy: 0.924\n",
      "\t\tTrain loss: 0.321.. \t\tTest loss: 0.344.. \t\tTest accuracy: 0.918\n",
      "\t\tTrain loss: 0.308.. \t\tTest loss: 0.297.. \t\tTest accuracy: 0.927\n",
      "\t\tTrain loss: 0.324.. \t\tTest loss: 0.373.. \t\tTest accuracy: 0.914\n",
      "\t\tTrain loss: 0.378.. \t\tTest loss: 0.364.. \t\tTest accuracy: 0.905\n",
      "\t\tTrain loss: 0.370.. \t\tTest loss: 0.388.. \t\tTest accuracy: 0.905\n",
      "\t\tTrain loss: 0.425.. \t\tTest loss: 0.338.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.342.. \t\tTest loss: 0.342.. \t\tTest accuracy: 0.914\n",
      "\t\tTrain loss: 0.447.. \t\tTest loss: 0.353.. \t\tTest accuracy: 0.911\n",
      "\t\tTrain loss: 0.320.. \t\tTest loss: 0.328.. \t\tTest accuracy: 0.917\n",
      "\t\tTrain loss: 0.358.. \t\tTest loss: 0.333.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.375.. \t\tTest loss: 0.337.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.357.. \t\tTest loss: 0.333.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.389.. \t\tTest loss: 0.369.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.360.. \t\tTest loss: 0.351.. \t\tTest accuracy: 0.913\n",
      "\t\tTrain loss: 0.398.. \t\tTest loss: 0.322.. \t\tTest accuracy: 0.924\n",
      "\t\tTrain loss: 0.359.. \t\tTest loss: 0.339.. \t\tTest accuracy: 0.916\n",
      "\t\tTrain loss: 0.352.. \t\tTest loss: 0.328.. \t\tTest accuracy: 0.921\n",
      "\t\tTrain loss: 0.429.. \t\tTest loss: 0.498.. \t\tTest accuracy: 0.871\n",
      "\t\tTrain loss: 0.435.. \t\tTest loss: 0.546.. \t\tTest accuracy: 0.882\n",
      "\t\tTrain loss: 0.411.. \t\tTest loss: 0.353.. \t\tTest accuracy: 0.917\n"
     ]
    }
   ],
   "source": [
    "TEACHERS = 10\n",
    "teacher_dict = {}\n",
    "for teacher in range(TEACHERS):\n",
    "    \n",
    "    model = MNISTClassifier().to(device)\n",
    "    \n",
    "    \n",
    "    # Training metrics\n",
    "    EPOCHS = np.random.randint(5)\n",
    "    criterion = NLLLoss()\n",
    "    learning_rates = [0.001, 0.002, 0.003, 0.01, 0.0001, 0.0005]\n",
    "    learning_rate = learning_rates[np.random.randint(len(learning_rates))]\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    print_every = 100\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(\"TRAINING MODEL \",teacher)\n",
    "        print(f\"\\tEpoch {epoch+1}/{EPOCHS}.. \")\n",
    "        \n",
    "        # releasing unceseccary memory in GPU\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        for inputs, labels in trainloader:\n",
    "            steps += 1\n",
    "            \n",
    "            # Move input and label tensors to the default device\n",
    "            X, y = inputs.to(device), labels.to(device)\n",
    "\n",
    "            ########################\n",
    "            # TRAIN\n",
    "            ########################\n",
    "            optimizer.zero_grad()\n",
    "            logps = model.forward(X)\n",
    "            loss = criterion(logps, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "                ########################\n",
    "                # TEST\n",
    "                ########################\n",
    "                model.eval()\n",
    "                \n",
    "                test_loss = 0\n",
    "                accuracy = 0\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in testloader:\n",
    "                        X, y = inputs.to(device), labels.to(device)\n",
    "                        logps = model.forward(X)\n",
    "                        batch_loss = criterion(logps, y)\n",
    "                        test_loss += batch_loss.item()\n",
    "\n",
    "                        # Calculate accuracy\n",
    "                        ps = torch.exp(logps)\n",
    "                        top_p, top_class = ps.topk(1, dim=1)\n",
    "                        equals = top_class == y.view(*top_class.shape)\n",
    "                        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "                print(f\"\\t\\tTrain loss: {running_loss/print_every:.3f}.. \"\n",
    "                      f\"\\t\\tTest loss: {test_loss/len(testloader):.3f}.. \"\n",
    "                      f\"\\t\\tTest accuracy: {accuracy/len(testloader):.3f}\")\n",
    "\n",
    "                ########################\n",
    "                # REPEAT\n",
    "                ########################\n",
    "                model.train()\n",
    "                running_loss = 0\n",
    "                \n",
    "    teacher_dict[teacher] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "for i in teacher_dict.keys():\n",
    "    torch.save(teacher_dict[i].state_dict(), './models/model_'+str(i)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL NO. 0\n",
      "\tTest loss: 0.188\tTest accuracy: 0.939\n",
      "MODEL NO. 1\n",
      "\tTest loss: 0.351\tTest accuracy: 0.913\n",
      "MODEL NO. 2\n",
      "\tTest loss: 0.109\tTest accuracy: 0.967\n",
      "MODEL NO. 3\n",
      "\tTest loss: 2.307\tTest accuracy: 0.096\n",
      "MODEL NO. 4\n",
      "\tTest loss: 0.304\tTest accuracy: 0.922\n",
      "MODEL NO. 5\n",
      "\tTest loss: 2.307\tTest accuracy: 0.109\n",
      "MODEL NO. 6\n",
      "\tTest loss: 0.125\tTest accuracy: 0.961\n",
      "MODEL NO. 7\n",
      "\tTest loss: 0.435\tTest accuracy: 0.876\n",
      "MODEL NO. 8\n",
      "\tTest loss: 0.159\tTest accuracy: 0.953\n",
      "MODEL NO. 9\n",
      "\tTest loss: 0.353\tTest accuracy: 0.917\n"
     ]
    }
   ],
   "source": [
    "# Double check accuracy\n",
    "for i in teacher_dict.keys():\n",
    "    print('MODEL NO. '+str(i))\n",
    "    model = teacher_dict[i]\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            X, y = inputs.to(device), labels.to(device)\n",
    "            logps = model.forward(X)\n",
    "            batch_loss = criterion(logps, y)\n",
    "            test_loss += batch_loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            ps = torch.exp(logps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = top_class == y.view(*top_class.shape)\n",
    "            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "    print(f\"\\tTest loss: {test_loss/len(testloader):.3f}\"\n",
    "          f\"\\tTest accuracy: {accuracy/len(testloader):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\\model_0.pt\n",
      "models\\model_1.pt\n",
      "models\\model_2.pt\n",
      "models\\model_3.pt\n",
      "models\\model_4.pt\n",
      "models\\model_5.pt\n",
      "models\\model_6.pt\n",
      "models\\model_7.pt\n",
      "models\\model_8.pt\n",
      "models\\model_9.pt\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "teacher_dict = {}\n",
    "for i, model_file in enumerate(os.listdir('models')):\n",
    "    model = MNISTClassifier().to(device)\n",
    "    path = os.path.join('models',model_file)\n",
    "    print(path)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    teacher_dict[i] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_results = []\n",
    "with torch.no_grad():\n",
    "    # For each model...\n",
    "    for i,k in enumerate(teacher_dict.keys()):\n",
    "        ph = torch.zeros(0).to(device).to(torch.long)\n",
    "        teacher_results.append(ph)\n",
    "        model = teacher_dict[k]\n",
    "        model.eval()\n",
    "        # loop through images\n",
    "        for inputs, labels in testloader:\n",
    "            X, y = inputs.to(device), labels.to(device)\n",
    "            logps = model.forward(X)\n",
    "            ps = torch.exp(logps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            # accumulate results\n",
    "            teacher_results[i] = torch.cat((teacher_results[i], top_class))\n",
    "\n",
    "# Assemble in to tensor\n",
    "teacher_results = torch.stack(teacher_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove superfluous dimension\n",
    "teacher_results = teacher_results.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permute so first index refers to image number\n",
    "preds = teacher_results.permute([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 10]), torch.Size([10, 10000]))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape, teacher_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 10\n",
    "for an_image in preds:\n",
    "    label_counts = np.bincount(an_image, minlength=num_labels)\n",
    "    epsilon = 0.1\n",
    "    beta = 1 / epsilon\n",
    "    for i in range(len(label_counts)):\n",
    "        label_counts[i] += np.random.laplace(0, beta, 1)\n",
    "    new_label = np.argmax(label_counts)\n",
    "    new_labels.append(new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 10\n",
    "epsilon = 1\n",
    "beta = 1 / epsilon\n",
    "ph = torch.zeros(0).to(device).to(torch.long)\n",
    "noisy_labels = ph\n",
    "true_labels = ph\n",
    "for an_image in preds:\n",
    "    label_counts = torch.bincount(an_image, minlength=num_labels)\n",
    "    true_labels = torch.cat((true_labels,torch.argmax(label_counts).unsqueeze_(0)))\n",
    "    for i in range(len(label_counts)):\n",
    "        noise = np.random.laplace(0, beta, 1)\n",
    "        label_counts[i] += noise[0]\n",
    "    noisy_labels = torch.cat((noisy_labels, label_counts.unsqueeze_(0)))\n",
    "# noise_labels = noisy_labels.permute([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 10]), torch.Size([10000]))"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_labels.shape, true_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n",
      "l too large to compute sensitivity\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-207-e2e2d61049b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=teacher_results,\n\u001b[0;32m      2\u001b[0m                                                    \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                                                    noise_eps=epsilon, delta=1e-5)\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mdata_dep_eps\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mdata_ind_eps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jkorg\\anaconda3\\envs\\spai\\lib\\site-packages\\syft\\frameworks\\torch\\differential_privacy\\pate.py\u001b[0m in \u001b[0;36mperform_analysis\u001b[1;34m(teacher_preds, indices, noise_eps, delta, moments, beta)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         total_log_mgf_nm += np.array(\n\u001b[1;32m--> 234\u001b[1;33m             \u001b[1;33m[\u001b[0m\u001b[0mlogmgf_from_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts_mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_eps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ml_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m         )\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jkorg\\anaconda3\\envs\\spai\\lib\\site-packages\\syft\\frameworks\\torch\\differential_privacy\\pate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         total_log_mgf_nm += np.array(\n\u001b[1;32m--> 234\u001b[1;33m             \u001b[1;33m[\u001b[0m\u001b[0mlogmgf_from_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts_mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_eps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ml_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m         )\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jkorg\\anaconda3\\envs\\spai\\lib\\site-packages\\syft\\frameworks\\torch\\differential_privacy\\pate.py\u001b[0m in \u001b[0;36mlogmgf_from_counts\u001b[1;34m(counts, noise_eps, l)\u001b[0m\n\u001b[0;32m    140\u001b[0m   \"\"\"\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m     \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_q_noisy_max\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_eps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlogmgf_exact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnoise_eps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jkorg\\anaconda3\\envs\\spai\\lib\\site-packages\\syft\\frameworks\\torch\\differential_privacy\\pate.py\u001b[0m in \u001b[0;36mcompute_q_noisy_max\u001b[1;34m(counts, noise_eps)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mgap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mq\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgap\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m4.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=teacher_results,\n",
    "                                                   indices=true_labels,\n",
    "                                                   noise_eps=epsilon, delta=1e-5)\n",
    "\n",
    "assert data_dep_eps < data_ind_eps\n",
    "\n",
    "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
    "print(\"Data Dependent Epsilon:\", data_dep_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 10000), (10000,))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.random.rand(10, 10000) * 10).astype(int).shape,(np.random.rand(10000) * 10).astype(int).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
