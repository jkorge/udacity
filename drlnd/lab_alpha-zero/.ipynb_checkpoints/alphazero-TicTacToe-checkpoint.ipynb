{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup a game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import random\n",
    "from collections import deque\n",
    "from copy import copy\n",
    "from math import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from ConnectN import ConnectN\n",
    "from Play import Play\n",
    "import MCTS\n",
    "from MCTS import device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_setting = {'size':(3,3), 'N':3}\n",
    "game = ConnectN(**game_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "1\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "game.move((0,1))\n",
    "print(game.state)\n",
    "print(game.player)\n",
    "print(game.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 0.]\n",
      " [1. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "1\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# player -1 move\n",
    "game.move((0,0))\n",
    "# player +1 move\n",
    "game.move((1,1))\n",
    "# player -1 move\n",
    "game.move((1,0))\n",
    "# player +1 move\n",
    "game.move((2,1))\n",
    "\n",
    "print(game.state)\n",
    "print(game.player)\n",
    "print(game.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play a game interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive plots not working\n",
    "# gameplay = Play(ConnectN(**game_setting),\n",
    "#               player1=None,\n",
    "#               player2=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize an AI to play the game\n",
    "We need to define a policy for tic-tac-toe, that takes the game state as input, and outputs a policy and a critic\n",
    "\n",
    "## Tentative Exercise:\n",
    "Code up your own policy for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        # solution\n",
    "        self.conv = nn.Conv2d(1, 16, kernel_size=2, stride=1, bias=False)\n",
    "        self.size = 2*2*16\n",
    "        self.fc = nn.Linear(self.size, 32)\n",
    "\n",
    "        # layers for the policy\n",
    "        self.fc_action1 = nn.Linear(32, 16)\n",
    "        self.fc_action2 = nn.Linear(16, 9)\n",
    "\n",
    "        # layers for the critic\n",
    "        self.fc_value1 = nn.Linear(32, 8)\n",
    "        self.fc_value2 = nn.Linear(8, 1)\n",
    "        self.tanh_value = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # solution\n",
    "        y = F.relu(self.conv(x))\n",
    "        y = y.view(-1, self.size)\n",
    "        y = F.relu(self.fc(y))\n",
    "\n",
    "        # the action head\n",
    "        a = F.relu(self.fc_action1(y))\n",
    "        a = self.fc_action2(a)\n",
    "        # availability of moves\n",
    "        avail = (torch.abs(x.squeeze()) != 1).type(torch.FloatTensor)\n",
    "        avail = avail.reshape(-1, 9).to(device)\n",
    "\n",
    "        # locations where actions are not possible, we set the prob to zero\n",
    "        maxa = torch.max(a)\n",
    "        # subtract off max for numerical stability (avoids blowing up at infinity)\n",
    "        exp = avail * torch.exp(a - maxa)\n",
    "        prob = exp / torch.sum(exp)\n",
    "\n",
    "        # the value head\n",
    "        value = F.relu(self.fc_value1(y))\n",
    "        value = self.tanh_value(self.fc_value2(value))\n",
    "\n",
    "        return prob.view(3,3).to(device), value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a player that uses MCTS and the expert policy + critic to play a game\n",
    "\n",
    "We've introduced a new parameter\n",
    "$T$ = temperature\n",
    "\n",
    "This tells us how to choose the next move based on the MCTS results\n",
    "\n",
    "$$p_a = \\frac{N_a^{\\frac{1}{T}}}{\\sum_a N_a^{\\frac{1}{T}}}$$\n",
    "\n",
    "$T \\rightarrow 0$, we choose action with largest $N_a$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy().to(device)\n",
    "def Policy_Player_MCTS(game):\n",
    "    mytree = MCTS.Node(copy(game))\n",
    "    for _ in range(50):\n",
    "        mytree.explore(policy)\n",
    "    mytreenext, (v, nn_v, p, nn_p) = mytree.next(temperature=0.1)\n",
    "    return mytreenext.game.last_move\n",
    "\n",
    "def Random_Player(game):\n",
    "    return random.choice(game.available_moves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "game = ConnectN(**game_setting)\n",
    "print(game.state)\n",
    "Policy_Player_MCTS(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play a game against the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gameplay=Play(ConnectN(**game_setting), \n",
    "#               player1=None, \n",
    "#               player2=Policy_Player_MCTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our alphazero agent and optimizer\n",
    "game = ConnectN(**game_setting)\n",
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01, weight_decay=1.0e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tenative exercise:\n",
    "code up the alphazero loss function, defined to be\n",
    "$$L = \\sum_t \\left\\{ (v^{(t)}_\\theta - z)^2  - \\sum_a \\left\\{ p^{(t)}_a \\log \\pi_\\theta(a|s_t) + \\textrm{constant} \\right\\} \\right\\}$$ \n",
    "I added a constant term $\\sum_t \\sum_a p^{(t)}\\log p^{(t)}$ so that when $v_\\theta^{(t)} = z$ and $p^{(t)}_a = \\pi_\\theta(a|s_t)$, $L=0$, this way we can have some metric of progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                       | 0/400 [00:12<?, ?episode/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-08bf0c5e08ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mmytree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutcome\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                 \u001b[0mmytree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mcurrent_player\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmytree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\udrl\\lab_alpha-zero\\MCTS.py\u001b[0m in \u001b[0;36mexplore\u001b[1;34m(self, policy)\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[1;31m# policy outputs results from the perspective of the next player\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[1;31m# thus extra - sign is needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m             \u001b[0mnext_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m             \u001b[0mcurrent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[0mcurrent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_child\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\udrl\\lab_alpha-zero\\MCTS.py\u001b[0m in \u001b[0;36mprocess_policy\u001b[1;34m(policy, game)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;31m# we add a negative sign because when deciding next move,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;31m# the current player is the previous player making the move\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavailable_moves\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtinv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mNode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train our agent\n",
    "episodes = 400\n",
    "episode = 0\n",
    "outcomes = []\n",
    "losses = []\n",
    "\n",
    "with tqdm(total=episodes, unit='episode') as pbar:\n",
    "    for episode in range(episodes):\n",
    "\n",
    "        mytree = MCTS.Node(ConnectN(**game_setting))\n",
    "        vterm = []\n",
    "        logterm = []\n",
    "\n",
    "        pbar.set_description('Exploring', refresh=True)\n",
    "        while mytree.outcome is None:\n",
    "            for _ in range(50):\n",
    "                mytree.explore(policy)\n",
    "\n",
    "        current_player = mytree.game.player\n",
    "        pbar.set_description('Searching', refresh=True)\n",
    "        mytree, (v, nn_v, p, nn_p) = mytree.next()        \n",
    "        mytree.detach_mother()\n",
    "\n",
    "        ##############################\n",
    "        # solution\n",
    "        # compute prob * log(pi)\n",
    "        pbar.set_description('Computing loss', refresh=True)\n",
    "        loglist = torch.log(nn_p) * p\n",
    "\n",
    "        # constant term to make sure if policy result = MCTS result, loss = 0\n",
    "        constant = torch.where(p>0, p * torch.log(p), torch.tensor(0.))\n",
    "\n",
    "        logterm.append(-torch.sum(loglist - constant))\n",
    "        vterm.append(nn_v * current_player)\n",
    "        ##############################\n",
    "\n",
    "        # we compute the \"policy_loss\" for computing gradient\n",
    "        outcome = mytree.outcome\n",
    "        outcomes.append(outcome)\n",
    "\n",
    "        ##############################\n",
    "        # solution\n",
    "        loss = torch.sum((torch.stack(vterm)-outcome)**2 + torch.stack(logterm))\n",
    "        ##############################\n",
    "        \n",
    "        pbar.set_description('Backpropagating', refresh=True)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        losses.append(float(loss))\n",
    "        optimizer.step()\n",
    "\n",
    "        # graph of current iteration remains as long as loss is in scope\n",
    "        # del to remove from scope and release computation graph\n",
    "        del loss\n",
    "\n",
    "        pbar.set_postfix({'loss': np.mean(losses[-20:])})\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot your losses\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play a game against your alphazero agent !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as first player\n",
    "gameplay=Play(ConnectN(**game_setting), \n",
    "              player1=None, \n",
    "              player2=Policy_Player_MCTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as second player\n",
    "gameplay=Play(ConnectN(**game_setting), \n",
    "              player2=None, \n",
    "              player1=Policy_Player_MCTS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
