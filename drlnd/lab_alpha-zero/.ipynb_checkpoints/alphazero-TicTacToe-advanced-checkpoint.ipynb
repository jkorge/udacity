{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize a game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import random\n",
    "from collections import deque\n",
    "from copy import copy\n",
    "from math import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from ConnectN import ConnectN\n",
    "from Play import Play\n",
    "import MCTS\n",
    "from MCTS import device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_setting = {'size':(6,6), 'N':4, 'pie_rule':True}\n",
    "game = ConnectN(**game_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameplay=Play(ConnectN(**game_setting),\n",
    "              player1=None,\n",
    "              player2=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our policy\n",
    "\n",
    "Please go ahead and define your own policy! See if you can train it under 1000 games and with only 1000 steps of exploration in each move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, game):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        # input = 6x6 board\n",
    "        # convert to 5x5x8\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=2, stride=1, bias=False)\n",
    "        # 5x5x16 to 3x3x32\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, bias=False)\n",
    "\n",
    "        self.size = 3 * 3 * 32\n",
    "        \n",
    "        # the part for actions\n",
    "        self.fc_action1 = nn.Linear(self.size, self.size//4)\n",
    "        self.fc_action2 = nn.Linear(self.size//4, 36)\n",
    "        \n",
    "        # the part for the value function\n",
    "        self.fc_value1 = nn.Linear(self.size, self.size//6)\n",
    "        self.fc_value2 = nn.Linear(self.size//6, 1)\n",
    "        self.tanh_value = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        y = F.leaky_relu(self.conv1(x))\n",
    "        y = F.leaky_relu(self.conv2(y))\n",
    "        y = y.view(-1, self.size)\n",
    "        \n",
    "        # action head\n",
    "        a = self.fc_action2(F.leaky_relu(self.fc_action1(y)))\n",
    "        \n",
    "        avail = (torch.abs(x.squeeze())!=1).type(torch.FloatTensor)\n",
    "        avail = avail.view(-1, 36)\n",
    "        maxa = torch.max(a)\n",
    "        exp = avail*torch.exp(a-maxa)\n",
    "        prob = exp/torch.sum(exp)\n",
    "        \n",
    "        # value head\n",
    "        value = self.tanh_value(self.fc_value2(F.leaky_relu( self.fc_value1(y) )))\n",
    "        return prob.view(6,6), value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a MCTS player for Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_setting = {'size':(6,6), 'N':4}\n",
    "game = ConnectN(**game_setting)\n",
    "policy = Policy(game)\n",
    "\n",
    "def Policy_Player_MCTS(game):\n",
    "    mytree = MCTS.Node(copy(game))\n",
    "    for _ in range(1000):\n",
    "        mytree.explore(policy)       \n",
    "    mytreenext, (v, nn_v, p, nn_p) = mytree.next(temperature=0.1)\n",
    "    return mytreenext.game.last_move\n",
    "\n",
    "def Random_Player(game):\n",
    "    return random.choice(game.available_moves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play a game against a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameplay=Play(ConnectN(**game_setting),\n",
    "              player1=Policy_Player_MCTS,\n",
    "              player2=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our alphazero agent and optimizer\n",
    "game=ConnectN(**game_setting)\n",
    "policy = Policy(game)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01, weight_decay=1.0e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware, training is **VERY VERY** slow!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train our agent\n",
    "episodes = 2000\n",
    "outcomes = []\n",
    "policy_loss = []\n",
    "Nmax = 1000\n",
    "\n",
    "with tqdm(total=episodes, unit='episode') as pbar:\n",
    "    for episode in range(episodes):\n",
    "\n",
    "        mytree = MCTS.Node(game)\n",
    "        logterm = []\n",
    "        vterm = []\n",
    "\n",
    "        while mytree.outcome is None:\n",
    "            for _ in range(Nmax):\n",
    "                mytree.explore(policy)\n",
    "                if mytree.N >= Nmax:\n",
    "                    break\n",
    "\n",
    "            current_player = mytree.game.player\n",
    "            mytree, (v, nn_v, p, nn_p) = mytree.next()\n",
    "            mytree.detach_mother()\n",
    "\n",
    "            loglist = torch.log(nn_p)*p\n",
    "            constant = torch.where(p>0, p*torch.log(p), torch.tensor(0.).to(device))\n",
    "            logterm.append(-torch.sum(loglist-constant))\n",
    "\n",
    "            vterm.append(nn_v*current_player)\n",
    "\n",
    "        # we compute the \"policy_loss\" for computing gradient\n",
    "        outcome = mytree.outcome\n",
    "        outcomes.append(outcome)\n",
    "        loss = torch.sum((torch.stack(vterm)-outcome)**2 + torch.stack(logterm))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        policy_loss.append(float(loss))\n",
    "        if (episode%500) == 0:\n",
    "            torch.save(policy, f'6-6-4-pie-{episode:d}.mypolicy')\n",
    "\n",
    "        del loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "Pit the AI you just trained against the provided policy ('6-6-4-pie.policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_policy = torch.load('6-6-4-pie.policy')\n",
    "\n",
    "def Challenge_Player_MCTS(game):\n",
    "    mytree = MCTS.Node(copy(game))\n",
    "    for _ in range(1000):\n",
    "        mytree.explore(challenge_policy)\n",
    "    mytreenext, (v, nn_v, p, nn_p) = mytree.next(temperature=0.1)\n",
    "    return mytreenext.game.last_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the game begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameplay=Play(ConnectN(**game_setting),\n",
    "              player2=Policy_Player_MCTS,\n",
    "              player1=Challenge_Player_MCTS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
